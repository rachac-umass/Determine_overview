{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44881d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost catboost polars optuna -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98f59d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn-intelex -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7535fe9",
   "metadata": {},
   "source": [
    "# Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bd0dc5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n",
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import optuna\n",
    "import pickle\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "import numpy as np\n",
    "## Enabling intel optimizations to \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8921a6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,f1_score, roc_auc_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3f89bf",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "498447f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def diag_med_lab_pid_exist_check(modeling_pids, diag_pid, medications_pid, lab_pid,age_data = None):\n",
    "    # Convert sets of pids for faster lookup\n",
    "    diag_pid_set = set(diag_pid)\n",
    "    medications_pid_set = set(medications_pid)\n",
    "    lab_pid_set = set(lab_pid)\n",
    "\n",
    "    # Create the result list using a single loop\n",
    "    if age_data:\n",
    "        result = [\n",
    "        f\"{age}_{int(pid in diag_pid_set)}{int(pid in medications_pid_set)}{int(pid in lab_pid_set)}\"\n",
    "        for pid,age in zip(modeling_pids,age_data)\n",
    "    ]\n",
    "    else:\n",
    "        result = [\n",
    "            f\"{int(pid in diag_pid_set)}{int(pid in medications_pid_set)}{int(pid in lab_pid_set)}\"\n",
    "            for pid in modeling_pids\n",
    "        ]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a10904c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix\n",
    "\n",
    "def get_metrics(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba[:,1])\n",
    "    print(\"AUC: \", roc_auc)\n",
    "\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "\n",
    "    # Calculate Sensitivity and Specificity\n",
    "    sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0  # handle division by zero\n",
    "    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0  # handle division by zero\n",
    "    \n",
    "    print(\"Sensitivity (Recall):\", sensitivity)\n",
    "    print(\"Specificity:\", specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7539dda0",
   "metadata": {},
   "source": [
    "# Config class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07f75d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ml_config:\n",
    "    base_folder ='../Determine_final_modeling_datasets/'\n",
    "    columns_to_ignore_cat = ['PATIENT_NUM','FirstOutcomeDate','Outcome']\n",
    "    target_column = 'Outcome'\n",
    "    z_bmi_cvs_file = 'Determine_joined_med_usage_lab_latest_diag_phe_with_icd10z_bmi_bp_cvs_ordinal_nominal_encoded.parquet'\n",
    "    without_z_bmi_file = 'Determine_joined_med_usage_lab_latest_diag_phe_without_icd10z_bmi_bp__ordinal_nominal_encoded.parquet'\n",
    "    with_z_bmi_file = 'Determine_joined_med_usage_lab_latest_diag_phe_with_icd10z_bmi_bp__ordinal_nominal_encoded.parquet'\n",
    "    ignore_med_patients = 'pat_num_ignore_meds.pkl'\n",
    "    \n",
    "    drop_low_feature_pids = False\n",
    "    \n",
    "\n",
    "# model1_df = pl.scan_parquet(ml_config.base_folder + ml_config.z_bmi_cvs_file)\n",
    "# model2_df = pl.scan_parquet(ml_config.base_folder + ml_config.without_z_bmi_file)\n",
    "# model3_df = pl.scan_parquet(ml_config.base_folder + ml_config.with_z_bmi_file)\n",
    "    \n",
    "# print(len(model1_df.collect()))\n",
    "# print(len(model2_df.collect()))\n",
    "# print(len(model3_df.collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef9240f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../Error_analysis_files/error_analysis_1_feature_pids.pkl\",'rb') as f:\n",
    "    pids_1feature = pickle.load(f)\n",
    "\n",
    "with open(\"../Error_analysis_files/error_analysis_3_feature_pids.pkl\",'rb') as f:\n",
    "    pids_3feature = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb67cee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ml_config.ignore_med_patients,'rb') as f:\n",
    "    ignore_med_pat = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79ec510",
   "metadata": {},
   "source": [
    "# Loading modeling data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c2e86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_config.file = ml_config.z_bmi_cvs_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e329a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_df_1 = pl.read_parquet(ml_config.base_folder + ml_config.without_z_bmi_file)\n",
    "modeling_df_2 = pl.read_parquet(ml_config.base_folder + ml_config.z_bmi_cvs_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0083022",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_df_1 = modeling_df_1.filter(~pl.col('PATIENT_NUM').is_in(ignore_med_pat))\n",
    "modeling_df_2 = modeling_df_2.filter(~pl.col('PATIENT_NUM').is_in(ignore_med_pat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549051f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(modeling_df_1['Outcome'].to_list(),return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219d5e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(modeling_df_2['Outcome'].to_list(),return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6008529e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modeling_df  = modeling_df.drop(ml_config.columns_to_drop)\n",
    "# modeling_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb646dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining categorical columns\n",
    "cat_features = [col for col in modeling_df_1.columns if  not (col.startswith('LOINC') \n",
    "                                                            or col in ml_config.columns_to_ignore_cat\n",
    "                                                            or col in ['BMI',\n",
    "                                                                       'mode_height',\n",
    "                                                                       'average_weight',\n",
    "                                                                         'average_diastolic_value',\n",
    "                                                                         'average_systolic_value',\n",
    "                                                                          \"ACS_MedHHIncome\", \n",
    "                                                                       \"ACS_GINI\", \n",
    "                                                                       \"ACS_Unemployment\", \n",
    "                                                                       \"ACS_pctPoverty100\", \n",
    "                                                                       \"ACS_pctCollGrad\"]\n",
    "                                                                          )]\n",
    "cat_features[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240bd7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loinc_columns = [col for col in modeling_df_1.columns if col.startswith('LOINC')]\n",
    "len(loinc_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a1f2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_df_1 = modeling_df_1.with_columns([\n",
    "    pl.col(col).cast(pl.Float32)\n",
    "    for col in loinc_columns\n",
    "])\n",
    "\n",
    "modeling_df_2 = modeling_df_2.with_columns([\n",
    "    pl.col(col).cast(pl.Float32)\n",
    "    for col in loinc_columns\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1fa6ad",
   "metadata": {},
   "source": [
    "# Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e85e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving test pids\n",
    "# with open('test_data_pids.pkl', 'wb') as file: \n",
    "#     # A new file will be created \n",
    "#     pickle.dump(test_pids, file) \n",
    "    \n",
    "# with open('train_data_pids.pkl', 'wb') as file: \n",
    "#     # A new file will be created \n",
    "#     pickle.dump(train_pids, file) \n",
    "\n",
    "# Open the file in binary mode \n",
    "with open('train_data_pids.pkl', 'rb') as file: \n",
    "    train_pids = pickle.load(file) \n",
    "with open('test_data_pids.pkl', 'rb') as file: \n",
    "    test_pids = pickle.load(file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e9d2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_1 = modeling_df_1.filter(pl.col('PATIENT_NUM').is_in(train_pids))\n",
    "data_test_1 = modeling_df_1.filter(pl.col('PATIENT_NUM').is_in(test_pids))\n",
    "\n",
    "\n",
    "data_train_2 = modeling_df_2.filter(pl.col('PATIENT_NUM').is_in(train_pids))\n",
    "data_test_2 = modeling_df_2.filter(pl.col('PATIENT_NUM').is_in(test_pids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf15971",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(data_test_1['Outcome'].to_list(), return_counts =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63690250",
   "metadata": {},
   "outputs": [],
   "source": [
    "del modeling_df_1, modeling_df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebd2672",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1, y_train_1 = data_train_1.drop(['PATIENT_NUM','FirstOutcomeDate','Outcome']).to_pandas(), data_train_1['Outcome'].to_pandas()\n",
    "X_test_1, y_test_1 = data_test_1.drop(['PATIENT_NUM','FirstOutcomeDate','Outcome']).to_pandas(), data_test_1['Outcome'].to_pandas()\n",
    "\n",
    "X_train_2, y_train_2 = data_train_2.drop(['PATIENT_NUM','FirstOutcomeDate','Outcome']).to_pandas(), data_train_2['Outcome'].to_pandas()\n",
    "X_test_2, y_test_2 = data_test_2.drop(['PATIENT_NUM','FirstOutcomeDate','Outcome']).to_pandas(), data_test_2['Outcome'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fa1404",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebe1b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_train_1, data_train_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dea6d02",
   "metadata": {},
   "source": [
    "# T-tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07699687",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Assuming we have two trained models, model_1 and model_2\n",
    "model_1 = CatBoostClassifier().load_model('../Determine_trained_models/Catboost_Determine_joined_med_usage_lab_latest_diag_phe_without_icd10z_bmi_bp__ordinal_nominal_encoded.parquet')\n",
    "model_2 = CatBoostClassifier().load_model('../Determine_trained_models/Catboost_Determine_joined_med_usage_lab_latest_diag_phe_with_icd10z_bmi_bp_cvs_ordinal_nominal_encoded.parquet')\n",
    "\n",
    "# Train the models\n",
    "model_1.fit(X_train_1, y_train_1, verbose=0)\n",
    "model_2.fit(X_train_2, y_train_2, verbose=0)\n",
    "\n",
    "# Evaluate the models using cross-validation\n",
    "scores_1 = cross_val_score(model_1, X_test_1, y_test_1, cv=5, scoring='roc_auc')\n",
    "scores_2 = cross_val_score(model_2, X_test_2, y_test_2, cv=5, scoring='roc_auc')\n",
    "\n",
    "# Calculate mean accuracy\n",
    "mean_score_1 = np.mean(scores_1)\n",
    "mean_score_2 = np.mean(scores_2)\n",
    "\n",
    "print(f\"Mean accuracy for feature set 1: {mean_score_1}\")\n",
    "print(f\"Mean accuracy for feature set 2: {mean_score_2}\")\n",
    "\n",
    "# Perform paired t-test\n",
    "t_stat, p_value = ttest_rel(scores_1, scores_2)\n",
    "\n",
    "print(f\"T-statistic: {t_stat}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "# Interpretation\n",
    "alpha = 0.05\n",
    "if p_value <= alpha:\n",
    "    print(\"The difference in performance is statistically significant.\")\n",
    "else:\n",
    "    print(\"The difference in performance is not statistically significant.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff57cc4",
   "metadata": {},
   "source": [
    "# Models comparision AUC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1211dc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from catboost import CatBoostClassifier\n",
    "import xgboost as xgb\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f6bc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_model = CatBoostClassifier().load_model('../Determine_trained_models/Catboost_Determine_joined_med_usage_lab_latest_diag_phe_with_icd10z_bmi_bp_cvs_ordinal_nominal_encoded.parquet')\n",
    "# xgboost_model = xgb.XGBClassifier().load_model('../Determine_trained_models/XGBoost_Determine_joined_med_usage_lab_latest_diag_phe_with_icd10z_bmi_bp_cvs_ordinal_nominal_encoded.parquet')\n",
    "# with open('../Determine_trained_models/XGB_Determine_joined_med_usage_lab_latest_diag_phe_with_icd10z_bmi_bp_cvs_ordinal_nominal_encoded.pkl','rb') as f:\n",
    "#     xgboost_model = pickle.load(f)\n",
    "# with open('../Determine_trained_models/LR_Determine_joined_med_usage_lab_latest_diag_phe_with_icd10z_bmi_bp_cvs_ordinal_nominal_encoded.pkl','rb') as f:\n",
    "#     lr_model = pickle.load(f)\n",
    "# with open('../Determine_trained_models/RF_Determine_joined_med_usage_lab_latest_diag_phe_with_icd10z_bmi_bp_cvs_ordinal_nominal_encoded.pkl','rb') as f:\n",
    "#     rf_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8319385",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "modeling_df = pl.read_parquet(ml_config.base_folder + ml_config.z_bmi_cvs_file)\n",
    "modeling_df = modeling_df.filter(~pl.col('PATIENT_NUM').is_in(ignore_med_pat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d41734",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d8d94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loinc_columns = [col for col in modeling_df.columns if col.startswith('LOINC')]\n",
    "len(loinc_columns)\n",
    "modeling_df = modeling_df.with_columns([\n",
    "    pl.col(col).cast(pl.Float32)\n",
    "    for col in loinc_columns\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c52cb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapse(list_gender):\n",
    "    return np.argmax(list(list_gender.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89521f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_df = modeling_df.with_columns(pl.struct(['Gender_CD_GQ','Gender_CD_M','Gender_CD_TG','Gender_CD_W']).map_elements(collapse, return_dtype = pl.Int8).alias('Sensitive_target'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd005c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_data_pids.pkl', 'rb') as file: \n",
    "    train_pids = pickle.load(file) \n",
    "with open('test_data_pids.pkl', 'rb') as file: \n",
    "    test_pids = pickle.load(file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b3ef69",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = modeling_df.filter(pl.col('PATIENT_NUM').is_in(train_pids))\n",
    "data_test = modeling_df.filter(pl.col('PATIENT_NUM').is_in(test_pids))\n",
    "\n",
    "del modeling_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e524bd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get the sensitive feature\n",
    "GI_train_list = data_train['Sensitive_target'].to_list()\n",
    "GI_test_list = data_test['Sensitive_target'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709c07f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = data_train.drop(['PATIENT_NUM','FirstOutcomeDate','Outcome','Sensitive_target']).to_pandas(), data_train['Outcome'].to_pandas()\n",
    "X_test, y_test = data_test.drop(['PATIENT_NUM','FirstOutcomeDate','Outcome','Sensitive_target']).to_pandas(), data_test['Outcome'].to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d45590",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_train, data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487e0ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d048753",
   "metadata": {},
   "source": [
    "# Demogrpahic Parity Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700d129a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fairlearn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c7a2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.datasets import fetch_adult\n",
    "from fairlearn.postprocessing import ThresholdOptimizer, plot_threshold_optimizer\n",
    "from fairlearn.metrics import demographic_parity_ratio, equalized_odds_ratio\n",
    "from fairlearn.reductions import DemographicParity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467ee9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demographic_parity_ratio(y_true, y_pred, sensitive_feature):\n",
    "    # Convert to a DataFrame for easier handling\n",
    "    results = pd.DataFrame({'y_true': y_true, 'y_pred': y_pred, 'sensitive': sensitive_feature})\n",
    "\n",
    "    # Calculate positive rate for each subgroup\n",
    "    positive_rates = results.groupby('sensitive').apply(\n",
    "        lambda x: np.mean(x['y_pred'])\n",
    "    )\n",
    "\n",
    "    # Calculate demographic parity ratio\n",
    "    dp_ratio = positive_rates.min() / positive_rates.max()\n",
    "\n",
    "    return dp_ratio, positive_rates\n",
    "\n",
    "\n",
    "\n",
    "catboost_pred = catboost_model.predict(X_test)\n",
    "\n",
    "dp_ratio, positive_rates = demographic_parity_ratio(y_test, catboost_pred, GI_test_list)\n",
    "\n",
    "print(\"Demographic Parity Ratio:\", dp_ratio)\n",
    "print(\"Positive Rates by Group:\", positive_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da871774",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f9c317",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "127edead",
   "metadata": {},
   "source": [
    "### t-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842ea719",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_1 = cross_val_score(catboost_model, X_test, y_test, cv=5, scoring='roc_auc')\n",
    "scores_2 = cross_val_score(xgboost_model, X_test, y_test, cv=5, scoring='roc_auc')\n",
    "\n",
    "# Calculate mean accuracy\n",
    "mean_score_1 = np.mean(scores_1)\n",
    "mean_score_2 = np.mean(scores_2)\n",
    "\n",
    "print(f\"Mean accuracy for feature set 1: {mean_score_1}\")\n",
    "print(f\"Mean accuracy for feature set 2: {mean_score_2}\")\n",
    "\n",
    "# Perform paired t-test\n",
    "t_stat, p_value = ttest_rel(scores_1, scores_2)\n",
    "\n",
    "print(f\"T-statistic: {t_stat}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "# Interpretation\n",
    "alpha = 0.05\n",
    "if p_value <= alpha:\n",
    "    print(\"The difference in performance is statistically significant.\")\n",
    "else:\n",
    "    print(\"The difference in performance is not statistically significant.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac206752",
   "metadata": {},
   "source": [
    "### t-test end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69a183d",
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_prob = catboost_model.predict_proba(X_test)[:, 1]\n",
    "xgboost_prob = xgboost_model.predict_proba(X_test)[:, 1]\n",
    "rf_prob = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform only the specified columns\n",
    "X_train[loinc_columns] = scaler.fit_transform(X_train[loinc_columns])\n",
    "X_test[loinc_columns] = scaler.transform(X_test[loinc_columns])\n",
    "lr_prob = lr_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate ROC curves\n",
    "fpr_catboost, tpr_catboost, _ = roc_curve(y_test, catboost_prob)\n",
    "fpr_xgboost, tpr_xgboost, _ = roc_curve(y_test, xgboost_prob)\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, rf_prob)\n",
    "fpr_lr, tpr_lr, _ = roc_curve(y_test, lr_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5790b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting ROC curves\n",
    "# Get predictions\n",
    "\n",
    "\n",
    "# Calculate AUC\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate precision and recall\n",
    "precision_catboost, recall_catboost, _ = precision_recall_curve(y_test, catboost_prob)\n",
    "precision_xgboost, recall_xgboost, _ = precision_recall_curve(y_test, xgboost_prob)\n",
    "precision_rf, recall_rf, _ = precision_recall_curve(y_test, rf_prob)\n",
    "precision_lr, recall_lr, _ = precision_recall_curve(y_test, lr_prob)\n",
    "\n",
    "# Calculate AUC for Precision-Recall\n",
    "auc_pr_catboost = auc(recall_catboost, precision_catboost)\n",
    "auc_pr_xgboost = auc(recall_xgboost, precision_xgboost)\n",
    "auc_pr_rf = auc(recall_rf, precision_rf)\n",
    "auc_pr_lr = auc(recall_lr, precision_lr)\n",
    "\n",
    "# Plot Precision-Recall curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(recall_catboost, precision_catboost, label=f'CatBoost (AUC-PR = {auc_pr_catboost:.3f})')\n",
    "plt.plot(recall_xgboost, precision_xgboost, label=f'XGBoost (AUC-PR = {auc_pr_xgboost:.3f})')\n",
    "plt.plot(recall_rf, precision_rf, label=f'Random Forest (AUC-PR = {auc_pr_rf:.3f})')\n",
    "plt.plot(recall_lr, precision_lr, label=f'Logistic Regression (AUC-PR = {auc_pr_lr:.3f})')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve Comparison')\n",
    "plt.legend(loc='lower left')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a185b53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc23813c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a970721",
   "metadata": {},
   "source": [
    "# Models Confidence Interval AUC, precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47558a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix\n",
    "from sklearn.utils import resample\n",
    "\n",
    "def calculate_confidence_interval(data, confidence=0.95):\n",
    "    \"\"\"Calculate the confidence interval for a given array of data.\"\"\"\n",
    "    n = len(data)\n",
    "    mean = np.mean(data)\n",
    "    std_err = np.std(data, ddof=1) / np.sqrt(n)\n",
    "    margin_of_error = std_err * 1.96  # For 95% confidence level\n",
    "    return mean, mean - margin_of_error, mean + margin_of_error\n",
    "\n",
    "def get_metrics(y_true, probs, threshold=0.5):\n",
    "    \"\"\"Calculate AUC, sensitivity, specificity, PPV, and their confidence intervals.\"\"\"\n",
    "    auc_score, auc_low, auc_high = calculate_confidence_interval(resample_and_score(y_true, probs, roc_auc_score))\n",
    "\n",
    "    # Calculate optimal threshold\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, probs)\n",
    "    optimal_idx = np.argmax(tpr - fpr)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    \n",
    "    preds = (probs >= optimal_threshold).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, preds).ravel()\n",
    "    \n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    ppv = tp / (tp + fp)\n",
    "\n",
    "    sensitivity_scores = [score[0] for score in resample_and_score(y_true, probs, calculate_sensitivity_specificity, threshold=optimal_threshold)]\n",
    "    specificity_scores = [score[1] for score in resample_and_score(y_true, probs, calculate_sensitivity_specificity, threshold=optimal_threshold)]\n",
    "    ppv_scores = [score[2] for score in resample_and_score(y_true, probs, calculate_sensitivity_specificity, threshold=optimal_threshold)]\n",
    "    \n",
    "    sensitivity_mean, sens_low, sens_high = calculate_confidence_interval(sensitivity_scores)\n",
    "    specificity_mean, spec_low, spec_high = calculate_confidence_interval(specificity_scores)\n",
    "    ppv_mean, ppv_low, ppv_high = calculate_confidence_interval(ppv_scores)\n",
    "    \n",
    "    return (f\"{auc_score:.4f} ({auc_low:.4f} - {auc_high:.4f})\",\n",
    "            f\"{sensitivity_mean:.4f} ({sens_low:.4f} - {sens_high:.4f})\",\n",
    "            f\"{specificity_mean:.4f} ({spec_low:.4f} - {spec_high:.4f})\",\n",
    "            f\"{ppv_mean:.4f} ({ppv_low:.4f} - {ppv_high:.4f})\")\n",
    "\n",
    "def resample_and_score(y_true, probs, score_func, n_iterations=1000, **kwargs):\n",
    "    scores = []\n",
    "    for _ in range(n_iterations):\n",
    "        y_resampled, prob_resampled = resample(y_true, probs)\n",
    "        scores.append(score_func(y_resampled, prob_resampled, **kwargs))\n",
    "    return scores\n",
    "\n",
    "def calculate_sensitivity_specificity(y_true, probs, threshold):\n",
    "    preds = (probs >= threshold).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, preds).ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    ppv = tp / (tp + fp)\n",
    "    return sensitivity, specificity, ppv\n",
    "\n",
    "# Calculate metrics for each model\n",
    "metrics_catboost = get_metrics(y_test, catboost_prob)\n",
    "metrics_xgboost = get_metrics(y_test, xgboost_prob)\n",
    "metrics_lr = get_metrics(y_test, lr_prob)\n",
    "metrics_rf = get_metrics(y_test, rf_prob)\n",
    "\n",
    "# Create a DataFrame to display\n",
    "df_metrics = pd.DataFrame({\n",
    "    'Model': ['CatBoost', 'XGBoost', 'random forest','Logistic Regression'],\n",
    "    'AUC ROC (CI)': [metrics_catboost[0], metrics_xgboost[0], metrics_rf[0], metrics_lr[0]],\n",
    "    'Sensitivity (CI)': [metrics_catboost[1], metrics_xgboost[1], metrics_rf[1], metrics_lr[1]],\n",
    "    'Specificity (CI)': [metrics_catboost[2], metrics_xgboost[2], metrics_rf[2], metrics_lr[2]],\n",
    "    'PPV (CI)': [metrics_catboost[3], metrics_xgboost[3], metrics_rf[3], metrics_lr[3]],\n",
    "})\n",
    "\n",
    "print(df_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1c0a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29fa84b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
