{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "327f274c",
   "metadata": {},
   "source": [
    "# Installing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7d0591",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost catboost polars optuna shap -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad88592",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7583e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn-intelex -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a9e637",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mlflow -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d344a16b",
   "metadata": {},
   "source": [
    "# Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40228411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import optuna\n",
    "import pickle\n",
    "\n",
    "import joblib\n",
    "\n",
    "import numpy as np\n",
    "## Enabling intel optimizations to \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a9894b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120994b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,f1_score, roc_auc_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a9e2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173d0eb0",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4457c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def diag_med_lab_pid_exist_check(modeling_pids, diag_pid, medications_pid, lab_pid,age_data = None):\n",
    "    # Convert sets of pids for faster lookup\n",
    "    diag_pid_set = set(diag_pid)\n",
    "    medications_pid_set = set(medications_pid)\n",
    "    lab_pid_set = set(lab_pid)\n",
    "\n",
    "    # Create the result list using a single loop\n",
    "    if age_data:\n",
    "        result = [\n",
    "        f\"{age}_{int(pid in diag_pid_set)}{int(pid in medications_pid_set)}{int(pid in lab_pid_set)}\"\n",
    "        for pid,age in zip(modeling_pids,age_data)\n",
    "    ]\n",
    "    else:\n",
    "        result = [\n",
    "            f\"{int(pid in diag_pid_set)}{int(pid in medications_pid_set)}{int(pid in lab_pid_set)}\"\n",
    "            for pid in modeling_pids\n",
    "        ]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91111530",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix, average_precision_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_metrics(model, X_test, y_test, n_bootstrap=1000, random_state=42):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    print(\"AUC:\", roc_auc)\n",
    "\n",
    "    auc_pr = average_precision_score(y_test, y_pred_proba)\n",
    "    print(\"Precision-Recall AUC:\", auc_pr)\n",
    "\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "\n",
    "    sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "    ppv = TP / (TP + FP) if (TP + FP) > 0 else 0   # PPV calculation\n",
    "\n",
    "    print(\"Sensitivity (Recall):\", sensitivity)\n",
    "    print(\"Specificity:\", specificity)\n",
    "    print(\"PPV (Precision):\", ppv)\n",
    "\n",
    "    \n",
    "    def stratified_bootstrap(y_true, y_pred, y_pred_proba, n_bootstrap=1000, random_state=42):\n",
    "        pos_idx = np.where(y_true == 1)[0]\n",
    "        neg_idx = np.where(y_true == 0)[0]\n",
    "        n_pos, n_neg = int(len(pos_idx)/10), int(len(neg_idx)/10)\n",
    "        rng = np.random.default_rng(seed=random_state)\n",
    "\n",
    "        aucs, sensitivities, specificities, ppvs = [], [], [], []  # include ppvs\n",
    "\n",
    "        for _ in tqdm(range(n_bootstrap), desc=\"Bootstrapping\"):\n",
    "            pos_bs = rng.choice(pos_idx, size=n_pos, replace=True)\n",
    "            neg_bs = rng.choice(neg_idx, size=n_neg, replace=True)\n",
    "            idx = np.concatenate([pos_bs, neg_bs])\n",
    "            y_bs = np.array(y_true)[idx]\n",
    "            y_pred_bs = np.array(y_pred)[idx]\n",
    "            y_pred_proba_bs = np.array(y_pred_proba)[idx]\n",
    "\n",
    "            try:\n",
    "                auc = roc_auc_score(y_bs, y_pred_proba_bs)\n",
    "                aucs.append(auc)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            cm_bs = confusion_matrix(y_bs, y_pred_bs)\n",
    "            if cm_bs.shape != (2, 2): continue\n",
    "            TN_bs, FP_bs, FN_bs, TP_bs = cm_bs.ravel()\n",
    "            sens_bs = TP_bs / (TP_bs + FN_bs) if (TP_bs + FN_bs) > 0 else 0\n",
    "            spec_bs = TN_bs / (TN_bs + FP_bs) if (TN_bs + FP_bs) > 0 else 0\n",
    "            ppv_bs = TP_bs / (TP_bs + FP_bs) if (TP_bs + FP_bs) > 0 else 0   # PPV bootstrapped\n",
    "\n",
    "            sensitivities.append(sens_bs)\n",
    "            specificities.append(spec_bs)\n",
    "            ppvs.append(ppv_bs)  # store ppv\n",
    "\n",
    "        return aucs, sensitivities, specificities, ppvs\n",
    "\n",
    "    aucs, sensitivities, specificities, ppvs = stratified_bootstrap(y_test, y_pred, y_pred_proba, n_bootstrap, random_state)\n",
    "\n",
    "    def get_ci(data, alpha=0.05):\n",
    "        lower = np.percentile(data, 100 * (alpha/2))\n",
    "        upper = np.percentile(data, 100 * (1 - alpha/2))\n",
    "        return lower, upper\n",
    "\n",
    "    roc_auc_ci = get_ci(aucs)\n",
    "    sensitivity_ci = get_ci(sensitivities)\n",
    "    specificity_ci = get_ci(specificities)\n",
    "    ppv_ci = get_ci(ppvs)  # compute PPV CI\n",
    "\n",
    "    print(f\"ROC AUC 95% CI: {roc_auc_ci}\")\n",
    "    print(f\"Sensitivity 95% CI: {sensitivity_ci}\")\n",
    "    print(f\"Specificity 95% CI: {specificity_ci}\")\n",
    "    print(f\"PPV 95% CI: {ppv_ci}\")\n",
    "\n",
    "    return {\n",
    "        'confusion_matrix': cm,\n",
    "        'accuracy': accuracy,\n",
    "        'roc_auc': roc_auc, 'roc_auc_ci': roc_auc_ci,\n",
    "        'auc_pr': auc_pr,\n",
    "        'sensitivity': sensitivity, 'sensitivity_ci': sensitivity_ci,\n",
    "        'specificity': specificity, 'specificity_ci': specificity_ci,\n",
    "        'ppv': ppv, 'ppv_ci': ppv_ci,  # return PPV as well!\n",
    "        'classification_report': report\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0f0686",
   "metadata": {},
   "source": [
    "# Config class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7666974",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"../Determine_ML_FLOW_Experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05de974",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ml_config:\n",
    "    base_folder ='../Determine_final_modeling_datasets/'\n",
    "    columns_to_ignore_cat = ['PATIENT_NUM','FirstOutcomeDate','Outcome']\n",
    "    target_column = 'Outcome'\n",
    "    file = 'Determine_joined_med_usage_lab_median_domain_expert_diag_phemap_without_icd10z_bmi_bp_cvs_ordinal_nominal_encoded.parquet'\n",
    "    \n",
    "    patient_enc_info_path = '../Determine_cohort_after_visit_index_details.parquet'\n",
    "    \n",
    "    no_enc_ppid_experiment =  True\n",
    "    low_feature_count_less_than_1_bmi_missing_exclude = False\n",
    "    \n",
    "    no_enc_after_2_plus_vi_above_50_exlcude = False\n",
    "    \n",
    "    pat_before_index_visit_dia_meds_remove = True\n",
    "    \n",
    "    pat_outcome0_with_dia_meds_remove = True ### Dropping patients with active ingrident of diabetes medications\n",
    "    \n",
    "    sdoh_screening_experiment = False ### SDOH screening experiment flag\n",
    "    \n",
    "    replace_neg100_with_none = True\n",
    "    \n",
    "    \n",
    "    ### MlFlow Variables\n",
    "    mlflow_experiment_name = 'Low feature count experiments'\n",
    "    run_name = 'Effect of removing patient records who have less than 5 feature'\n",
    "    \n",
    "    model_name = 'catboost'\n",
    "    \n",
    "    with open('./Experiements_related_files/Outcome0_after_vi+1_no_enc_pids_over_50.pkl','rb') as f:\n",
    "        Outcome0_after_vi_1_no_enc_pids_over_50 = pickle.load(f)\n",
    "    \n",
    "    with open('./Experiements_related_files/Outcome0_after_vi+2_no_enc_pids_over_50.pkl','rb') as f:\n",
    "        Outcome0_after_vi_2_no_enc_pids_over_50 = pickle.load(f)\n",
    "        \n",
    "    with open('./Experiements_related_files/Patient_ids_feature_count_less_than_1_no_bmi.pkl','rb') as f:\n",
    "        Patient_ids_feature_count_less_than_1_no_bmi = pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fba56cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48b02885",
   "metadata": {},
   "source": [
    "# Loading modeling data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1394e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_df = pl.read_parquet(ml_config.base_folder + ml_config.file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3716fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'LOINC:2532-0' in modeling_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56204fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be43d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Replacing -100 with None\n",
    "if ml_config.replace_neg100_with_none:\n",
    "    print('Replacing -100 with None')\n",
    "    modeling_df = modeling_df.with_columns([\n",
    "        pl.when(pl.col(c) == -100).then(np.nan).otherwise(pl.col(c)).alias(c)\n",
    "        for c in modeling_df.columns\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb56e695",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of data points in the dataset: \",len(modeling_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9d1c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_df.filter(pl.col('Outcome')==1).sort('FirstOutcomeDate', descending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87fd71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_df.filter(pl.col('PATIENT_NUM') == 297249)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4b022c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(modeling_df['Outcome'].to_list(),return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664a3ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ml_config.pat_before_index_visit_dia_meds_remove:\n",
    "    with open('pat_num_ignore_dia_meds.pkl','rb') as f:\n",
    "        med_ignore_patient_num = pickle.load(f)\n",
    "    modeling_df = modeling_df.filter(~pl.col('PATIENT_NUM').is_in(med_ignore_patient_num))\n",
    "    print(len(modeling_df))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddddfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'metformin' in modeling_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72840f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ml_config.no_enc_ppid_experiment:\n",
    "    modeling_patient_ids = modeling_df['PATIENT_NUM'].to_list()\n",
    "    pids_enc_info_after_vi = pl.read_parquet(ml_config.patient_enc_info_path)['PATIENT_NUM'].to_list()\n",
    "    pids_to_drop = set(modeling_patient_ids) - set(pids_enc_info_after_vi)\n",
    "#     modeling_df = modeling_df.filter(((~pl.col('PATIENT_NUM').is_in(pids_to_drop)) & (pl.col('Outcome')==0)) | (pl.col('Outcome')==1))\n",
    "    print(\"Number of data points that needs to be excluded if seen in train: \",len(pids_to_drop))\n",
    "    print(\"The target distribution is:\", np.unique(modeling_df['Outcome'].to_list(),return_counts = True) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c7ab91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if ml_config.low_feature_count_exclude:\n",
    "    \n",
    "    \n",
    "if ml_config.pat_outcome0_with_dia_meds_remove:\n",
    "    with open('Determine_outcome0_act_ing_dia_after_index_visit_patient_nums.pkl', 'rb') as f:\n",
    "        pat_outcome0_with_dia_meds_remove = pickle.load(f)\n",
    "    print(\"Number of patient ids removed from dataset: \",len(pat_outcome0_with_dia_meds_remove))    \n",
    "    modeling_df = modeling_df.filter(~pl.col('PATIENT_NUM').is_in(pat_outcome0_with_dia_meds_remove)) \n",
    "    print(np.unique(modeling_df['Outcome'].to_list(),return_counts = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9214ffb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modeling_df  = modeling_df.drop(ml_config.columns_to_drop)\n",
    "# modeling_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dd7ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining categorical columns\n",
    "cat_features = [col for col in modeling_df.columns if  not (col.startswith('LOINC') \n",
    "                                                            or col in ml_config.columns_to_ignore_cat\n",
    "                                                            or col in ['BMI',\n",
    "                                                                       'mode_height',\n",
    "                                                                       'average_weight',\n",
    "                                                                         'average_diastolic_value',\n",
    "                                                                         'average_systolic_value',\n",
    "                                                                          \"ACS_MedHHIncome\", \n",
    "                                                                       \"ACS_GINI\", \n",
    "                                                                       \"ACS_Unemployment\", \n",
    "                                                                       \"ACS_pctPoverty100\", \n",
    "                                                                       \"ACS_pctCollGrad\"]\n",
    "                                                                          )]\n",
    "numerical_features = [col for col in modeling_df.columns if ((col.startswith('LOINC') \n",
    "#                                                             or col not in ml_config.columns_to_ignore_cat\n",
    "                                                            or col in ['BMI',\n",
    "                                                                       'mode_height',\n",
    "                                                                       'average_weight',\n",
    "                                                                         'average_diastolic_value',\n",
    "                                                                         'average_systolic_value',\n",
    "                                                                          \"ACS_MedHHIncome\", \n",
    "                                                                       \"ACS_GINI\", \n",
    "                                                                       \"ACS_Unemployment\", \n",
    "                                                                       \"ACS_pctPoverty100\", \n",
    "                                                                       \"ACS_pctCollGrad\"\n",
    "                                                                          ] + [\n",
    "                                                                            \"mode_height\",\n",
    "#                                                                             \"median_value\",\n",
    "#                                                                             \"slope_weight\",\n",
    "                                                                            \"BMI\",\n",
    "                                                                            \"median_diastolic_value\",\n",
    "#                                                                             \"slope_dia_bp\",\n",
    "                                                                            \"median_systolic_value\",\n",
    "#                                                                             \"slope_sys_bp\"\n",
    "                                                                        ])\n",
    "                                                            and (col not in ml_config.columns_to_ignore_cat))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230cb66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'average_weight' in cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e40e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of categorical features: \",len(cat_features))\n",
    "print(\"Number of numerical features: \",len(numerical_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21658db",
   "metadata": {},
   "outputs": [],
   "source": [
    "loinc_columns = [col for col in modeling_df.columns if col.startswith('LOINC')]\n",
    "print(\"Number of lab results features: \",len(loinc_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdc9d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "### VERY IMPORTANT!!!!\n",
    "modeling_df = modeling_df.with_columns([\n",
    "    pl.col(col).cast(pl.Float32)\n",
    "    for col in loinc_columns\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e61b246",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_df.filter(pl.col('metformin')==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0939c737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique data types\n",
    "unique_dtypes = set(modeling_df.dtypes)\n",
    "print(unique_dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3918330",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(modeling_df['Outcome'].to_list(),return_counts = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09862a3",
   "metadata": {},
   "source": [
    "# Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c014d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_pids, data_test_pids = train_test_split(modeling_df['PATIENT_NUM'], test_size=0.2, stratify=modeling_df['Outcome'], random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8877c12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_pids = data_train_pids.to_list()\n",
    "data_test_pids = data_test_pids.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db007681",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pids = data_train_pids\n",
    "test_pids = data_test_pids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3be3cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_train_pids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41aeb326",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_test_pids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9b64cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ml_config.sdoh_screening_experiment:\n",
    "\n",
    "    with open('sdohscreen-test_data_pids.pkl', 'wb') as file: \n",
    "        # A new file will be created \n",
    "        pickle.dump(data_test_pids, file) \n",
    "\n",
    "    with open('sdohscreen-train_data_pids.pkl', 'wb') as file: \n",
    "        # A new file will be created \n",
    "        pickle.dump(data_train_pids, file) \n",
    "\n",
    "    #Open the file in binary mode \n",
    "    with open('sdohscreen-train_data_pids.pkl', 'rb') as file: \n",
    "        train_pids = pickle.load(file) \n",
    "    with open('sdohscreen-test_data_pids.pkl', 'rb') as file: \n",
    "        test_pids = pickle.load(file)  \n",
    "    \n",
    "    \n",
    "else:\n",
    "    # saving test pids\n",
    "    with open('test_data_pids.pkl', 'wb') as file: \n",
    "        # A new file will be created \n",
    "        pickle.dump(data_test_pids, file) \n",
    "\n",
    "    with open('train_data_pids.pkl', 'wb') as file: \n",
    "        # A new file will be created \n",
    "        pickle.dump(data_train_pids, file) \n",
    "\n",
    "    \n",
    "    \n",
    "print(\"Number of train pids: \", len(train_pids))\n",
    "print(\"Number of test pids: \", len(test_pids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48f60d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ml_config.sdoh_screening_experiment:\n",
    "    print(\"In screening sdoh experiment\")\n",
    "#Open the file in binary mode \n",
    "    with open('sdohscreen-train_data_pids.pkl', 'rb') as file: \n",
    "        train_pids = pickle.load(file) \n",
    "    with open('sdohscreen-test_data_pids.pkl', 'rb') as file: \n",
    "        test_pids = pickle.load(file) \n",
    "    \n",
    "else:    \n",
    "    #Open the file in binary mode \n",
    "    with open('train_data_pids.pkl', 'rb') as file: \n",
    "        train_pids = pickle.load(file) \n",
    "    with open('test_data_pids.pkl', 'rb') as file: \n",
    "        test_pids = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1c7829",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_pids))\n",
    "print(len(test_pids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575fd220",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4f6256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_pids = list(set(data_train_pids+data_test_pids) - set(test_pids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9481a5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = modeling_df.filter(pl.col('PATIENT_NUM').is_in(train_pids))\n",
    "data_test = modeling_df.filter(pl.col('PATIENT_NUM').is_in(test_pids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37103936",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data_train))\n",
    "print(len(data_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0db74c",
   "metadata": {},
   "source": [
    "### Experiments exclusion part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec3579d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ml_config.no_enc_ppid_experiment:\n",
    "    print(\"Dropping patients with no encounters\")\n",
    "    data_train = data_train.filter(((~pl.col('PATIENT_NUM').is_in(pids_to_drop)) & (pl.col('Outcome')==0)) | (pl.col('Outcome')==1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67563ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ml_config.no_enc_after_2_plus_vi_above_50_exlcude:\n",
    "    print(\"Dropping patients with no encounters after bi +2 year and age over 50\")\n",
    "    data_train = data_train.filter(~pl.col('PATIENT_NUM').is_in(ml_config.Outcome0_after_vi_1_no_enc_pids_over_50 +\n",
    "                                                                ml_config.Outcome0_after_vi_2_no_enc_pids_over_50\n",
    "                                                               ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98754939",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ml_config.low_feature_count_less_than_1_bmi_missing_exclude:\n",
    "    \n",
    "    print(\"Dropping patients with fetaures less than 1 and bmi is missing\")\n",
    "    data_train = data_train.filter(~pl.col('PATIENT_NUM').is_in(ml_config.Patient_ids_feature_count_less_than_1_no_bmi))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d82f314",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data_train))\n",
    "print(len(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61249ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(data_test['Outcome'].to_list(), return_counts =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa42aecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "del modeling_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ac8918",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_firstoutcome_df = data_test.select(['PATIENT_NUM','FirstOutcomeDate','Outcome'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a86ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,y_train = data_train.drop(['PATIENT_NUM','FirstOutcomeDate','Outcome','mode_height']).to_pandas(), data_train['Outcome'].to_pandas()\n",
    "X_test,y_test = data_test.drop(['PATIENT_NUM','FirstOutcomeDate','Outcome', 'mode_height']).to_pandas(), data_test['Outcome'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b67dbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5065dcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_train#, data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00ee8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[['Sex_CD_F','Sex_CD_M']].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6259ca",
   "metadata": {},
   "source": [
    "# Boruta feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174ab292",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install boruta -q "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991c93c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loinc_columns = [col for col in X_train.columns if col.startswith('LOINC')]\n",
    "# len(loinc_columns)\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f3cb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from boruta import BorutaPy\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0496274c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train, X_train_2, y_train, y_train_2 = train_test_split(\n",
    "#     X_train, y_train,\n",
    "#     train_size=0.6,       # Or e.g. 0.2 if you want a % instead of raw size\n",
    "#     stratify=y_train,\n",
    "#     random_state=42\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d46b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = float(y_train.value_counts()[0]) / y_train.value_counts()[1]\n",
    "\n",
    "\n",
    "model = XGBClassifier(n_jobs=-1, n_estimators = 300, max_depth = 4, scale_pos_weight = ratio)\n",
    "\n",
    "#Initialize Boruta\n",
    "feat_selector = BorutaPy(verbose=2, estimator=model, max_iter=150)  # number of iterations to perform\n",
    "\n",
    "#Train Boruta\n",
    "#N.B.: X and y must be numpy arrays\n",
    "feat_selector.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca23c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = X_train.columns[feat_selector.support_]\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e2bb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5cafdc",
   "metadata": {},
   "source": [
    "# ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38625c1",
   "metadata": {},
   "source": [
    "## Single objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb01fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_boruta = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0a029d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_boruta:\n",
    "    boruta_features = ['Age_group', 'azithromycin', 'levothyroxine', 'acyclovir',\n",
    "       'ceftriaxone', 'phe_401.1', 'phe_271.3', 'phe_41.0', 'phe_278.11',\n",
    "       'phe_649.1', 'LOINC:2085-9', 'LOINC:2345-7', 'LOINC:74774-1',\n",
    "       'LOINC:27353-2', 'LOINC:9318-7', 'LOINC:62238-1', 'mode_height', 'BMI',\n",
    "       'median_diastolic_value', 'ACS_MedHHIncome', 'ACS_pctCollGrad',\n",
    "       'Race_CD_02', 'Race_CD_05', 'Hispanic_CD_Y', 'Gender_CD_M',\n",
    "       'Gender_CD_W']\n",
    "    boruta_features.remove('mode_height')\n",
    "    \n",
    "    X_train = X_train[boruta_features]\n",
    "    X_test =X_test[boruta_features]\n",
    "    \n",
    "    print(\"Number of featues being used: \",len(boruta_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5cb36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns starting with 'SDH'\n",
    "\n",
    "if ml_config.sdoh_screening_experiment:\n",
    "    print('SDH features excluded')\n",
    "    X_train = X_train.loc[:, ~X_train.columns.str.startswith('SDH')]\n",
    "    X_test = X_test.loc[:, ~X_test.columns.str.startswith('SDH')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee16a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa3986e",
   "metadata": {},
   "source": [
    "#### Adjusting features based on model_name (some models need feature scaling for numerical data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a316e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982d2cae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "numerical_features_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2e0500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the scaler\n",
    "\n",
    "if ml_config.model_name == 'logistic_regression':\n",
    "    numerical_features_use = [i for i in X_train.columns if i in numerical_features]\n",
    "    scaler = StandardScaler()\n",
    "    print(\"Scaling data\")\n",
    "    # Fit and transform only the specified columns\n",
    "    X_train[numerical_features_use] = scaler.fit_transform(X_train[numerical_features_use])\n",
    "    X_test[numerical_features_use] = scaler.transform(X_test[numerical_features_use])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d77216",
   "metadata": {},
   "source": [
    "### Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6391807",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_config.model_name = 'xgboost'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bd55e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "def objective(trial):\n",
    "    if ml_config.model_name == 'xgboost':\n",
    "    \n",
    "        param = {\n",
    "        'objective': 'binary:logistic',  # Binary classification\n",
    "        #'eval_metric': 'auc',             # Evaluation metric\n",
    "        'seed': 42,\n",
    "        'eta': trial.suggest_float('eta', 0.01, 0.3, step=0.01),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 600, step =100),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0, step=0.1),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0, step=0.1),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5.0),\n",
    "        'scale_pos_weight': (len(y_train) - sum(y_train)) / sum(y_train),  # Class weight for imbalance\n",
    "        }\n",
    "        \n",
    "        model = XGBClassifier(**param, enable_categorical=True, device=\"cuda\")\n",
    "\n",
    "    elif ml_config.model_name == 'catboost':\n",
    "        param = {\n",
    "            \"iterations\": trial.suggest_categorical('iterations',[200, 400, 600, 800]),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.1, log=True),\n",
    "            \"depth\": trial.suggest_int(\"depth\", 4, 12),\n",
    "            #\"subsample\": trial.suggest_float(\"subsample\", 0.05, 1.0),\n",
    "            #\"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.05, 1.0),\n",
    "            \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 1, 300),\n",
    "            'task_type':\"GPU\",\n",
    "        }\n",
    "        model = CatBoostClassifier(**param, auto_class_weights='Balanced',allow_writing_files=False,\n",
    "                                   silent=True)\n",
    "\n",
    "    elif ml_config.model_name == 'random_forest':\n",
    "        param = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 50, 800),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "        }\n",
    "        model = RandomForestClassifier(**param, class_weight ='balanced', n_jobs = -1)\n",
    "\n",
    "    elif ml_config.model_name == 'logistic_regression':\n",
    "        param = {\n",
    "            'C': trial.suggest_loguniform('C', 1e-2, 10.0),\n",
    "            'max_iter': trial.suggest_int('max_iter', 100, 600),\n",
    "            'solver': trial.suggest_categorical('solver', ['liblinear']),\n",
    "        }\n",
    "        model = LogisticRegression(**param,class_weight ='balanced', n_jobs = -1 )\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model name\")\n",
    "    \n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    recall1 = recall_score(y_test, y_pred, pos_label=1)\n",
    "    return recall1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247c996a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials= 10)\n",
    "\n",
    "print('')\n",
    "print('Best hyperparameters:', study.best_params)\n",
    "print('Best AUC:', study.best_value)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a5de71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_params =study.best_params\n",
    "\n",
    "if ml_config.model_name == 'xgboost':\n",
    "        \n",
    "    model = XGBClassifier(#**best_params, \n",
    "                          scale_pos_weight = (len(y_train) - sum(y_train)) / sum(y_train),  \n",
    "#                           use_label_encoder=False,\n",
    "                          device = 'cuda', \n",
    "                          )\n",
    "    \n",
    "elif ml_config.model_name == 'catboost':\n",
    "    model = CatBoostClassifier(#**best_params, \n",
    "                               auto_class_weights='Balanced',allow_writing_files=False,\n",
    "                                   task_type ='GPU',\n",
    "                                   silent=True)\n",
    "    \n",
    "elif ml_config.model_name == 'random_forest':\n",
    "    model = RandomForestClassifier(**best_params, \n",
    "                                   class_weight ='balanced', n_jobs = -1)\n",
    "    \n",
    "elif ml_config.model_name == 'logistic_regression':\n",
    "    model = LogisticRegression(**best_params, \n",
    "#                                solver = 'liblinear',\n",
    "                               class_weight ='balanced', n_jobs = -1)\n",
    "    \n",
    "else:\n",
    "    raise ValueError(\"Unsupported model name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dd1db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f033e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics = get_metrics(model,X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bafcbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010dce17",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Confusion matrix\n",
    "cm = all_metrics['confusion_matrix']\n",
    "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in cm.flatten()]\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in cm.flatten()/np.sum(cm)]\n",
    "labels = [f\"{v1}\\n{v2}\" for v1, v2 in zip(group_names, group_counts)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "fig, ax = plt.subplots()  # Use fig, ax instead of plt.subplot()\n",
    "sns.heatmap(cm, annot=labels, fmt='', cmap='Blues', ax=ax)\n",
    "\n",
    "ax.set_xlabel('Predicted labels')\n",
    "ax.set_ylabel('True labels')\n",
    "ax.set_title('Confusion Matrix') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8f6bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a380bc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model using joblib\n",
    "if use_boruta:\n",
    "    feat_use = 'boruta_features'\n",
    "else:\n",
    "    feat_use = 'all_features'\n",
    "\n",
    "model_filename_base = f\"../Determine_trained_models/{ml_config.model_name}_dataset_{ml_config.file.split('.')[0]}_{feat_use}\"\n",
    "\n",
    "if \"xgboost\" in ml_config.model_name:\n",
    "    # XGBoost - use built-in save_model\n",
    "    model_filename = model_filename_base + \".json\"\n",
    "    model.save_model(model_filename)\n",
    "    print(f\"XGBoost model saved as {model_filename}\")\n",
    "elif \"catboost\" in ml_config.model_name:\n",
    "    # CatBoost - use built-in save_model\n",
    "    model_filename = model_filename_base + \".cbm\"\n",
    "    model.save_model(model_filename)\n",
    "    print(f\"CatBoost model saved as {model_filename}\")\n",
    "elif  \"random_forest\" in ml_config.model_name:\n",
    "    # RandomForest (scikit-learn) - use joblib\n",
    "    model_filename = model_filename_base + \".pkl\"\n",
    "    dump(model, model_filename)\n",
    "    print(f\"RandomForest model saved as {model_filename}\")\n",
    "else:\n",
    "    # Default to joblib for 'sklearn' compatible models\n",
    "    model_filename = model_filename_base + \".pkl\"\n",
    "    dump(model, model_filename)\n",
    "    print(f\"Model saved as {model_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2f8e1c",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5c9d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = model.get_feature_importance()\n",
    "feature_names = X_train.columns  # Assuming X_train is a DataFrame\n",
    "\n",
    "# Combine feature names with their importances\n",
    "feature_importance_dict = dict(zip(feature_names, feature_importances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c04f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_not_ignored = [feature for feature, importance in feature_importance_dict.items() if importance != 0]\n",
    "\n",
    "print(\"Features that are not ignored:\", len(features_not_ignored))\n",
    "print(\"Actual feature count: \",len(feature_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a75821f",
   "metadata": {},
   "source": [
    "# Fairness Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a951372",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install fairlearn -q\n",
    "\n",
    "from fairlearn.metrics import MetricFrame, selection_rate, demographic_parity_difference, equalized_odds_difference\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aec05bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5a595b",
   "metadata": {},
   "outputs": [],
   "source": [
    "race_columns = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9606c7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6d9517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_collapse(row):\n",
    "    if row.sum() == 0:\n",
    "        return -1\n",
    "    return np.argmax(row)\n",
    "\n",
    "sensitive_feature_df = X_test[['Race_CD_01','Race_CD_02','Race_CD_03','Race_CD_04', 'Race_CD_05']]  \n",
    "\n",
    "\n",
    "\n",
    "sensitive_feature = sensitive_feature_df.apply(row_collapse, axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869e164b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('For RACE')\n",
    "metricframe = MetricFrame(\n",
    "    metrics={\n",
    "        'accuracy': accuracy_score,\n",
    "        'selection_rate': selection_rate\n",
    "    },\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred,\n",
    "    sensitive_features=sensitive_feature\n",
    ")\n",
    "\n",
    "print(metricframe.by_group)\n",
    "\n",
    "# Demographic parity difference (difference in positive rate between groups)\n",
    "dp_diff = demographic_parity_difference(y_test, y_pred, sensitive_features=sensitive_feature)\n",
    "print(\"Demographic Parity Difference:\", dp_diff)\n",
    "\n",
    "# Equalized odds difference (difference in TPR/FPR between groups)\n",
    "eo_diff = equalized_odds_difference(y_test, y_pred, sensitive_features=sensitive_feature)\n",
    "print(\"Equalized Odds Difference:\", eo_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87fd276",
   "metadata": {},
   "outputs": [],
   "source": [
    "[col for col in X_train.columns if 'Gender' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19ba2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitive_feature_df = X_test[[col for col in X_train.columns if 'Gender' in col]]\n",
    "\n",
    "\n",
    "\n",
    "sensitive_feature = sensitive_feature_df.apply(row_collapse, axis =1)\n",
    "print('For gender')\n",
    "metricframe = MetricFrame(\n",
    "    metrics={\n",
    "        'accuracy': accuracy_score,\n",
    "        'selection_rate': selection_rate\n",
    "    },\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred,\n",
    "    sensitive_features=sensitive_feature\n",
    ")\n",
    "\n",
    "print(metricframe.by_group)\n",
    "\n",
    "# Demographic parity difference (difference in positive rate between groups)\n",
    "dp_diff = demographic_parity_difference(y_test, y_pred, sensitive_features=sensitive_feature)\n",
    "print(\"Demographic Parity Difference:\", dp_diff)\n",
    "\n",
    "# Equalized odds difference (difference in TPR/FPR between groups)\n",
    "eo_diff = equalized_odds_difference(y_test, y_pred, sensitive_features=sensitive_feature)\n",
    "print(\"Equalized Odds Difference:\", eo_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d930da",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(sensitive_feature.to_list(), return_counts =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fb8015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demographic Parity Difference: 0.1147\n",
    "# What it means: The difference between the highest and lowest group selection rates.\n",
    "# Interpretation: The model is 11.47% more likely to assign the positive outcome to the most-favored group than the least-favored, when comparing all racial groups. Ideally, for fairness, this should be close to 0.\n",
    "# Equalized Odds Difference: 0.1308\n",
    "# What it means: Measures the largest difference in true positive rates (and possibly false positive rates) between groups.\n",
    "# Interpretation: A value of 0.13 (13.08%) means there is a noticeable disparity in how accurately the model assigns outcomes between groups. Again, closer to 0 is more fair."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29ea916",
   "metadata": {},
   "source": [
    "## Saving results and details to ml flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd15de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "\n",
    "class mlflow_config:\n",
    "    log_fairness_metrics = False\n",
    "    \n",
    "    log_feature_importance = False\n",
    "    \n",
    "    log_df_count_after_before = True\n",
    "    \n",
    "    \n",
    "def log_target_distribution(y, split_name):\n",
    "    counts = pd.Series(y).value_counts().sort_index()\n",
    "    fig, ax = plt.subplots(figsize=(6,4))\n",
    "    counts.plot(kind='bar', ax=ax)\n",
    "    ax.set_title(f'Target distribution for {split_name}')\n",
    "    ax.set_xlabel('Class')\n",
    "    ax.set_ylabel('Count')\n",
    "    for i, v in enumerate(counts):\n",
    "        ax.text(i, v + 0.01*max(counts), str(v), ha='center')\n",
    "    # Log the figure directly\n",
    "    mlflow.log_figure(fig, f\"{split_name}_target_distribution.png\")\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29baabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = model.get_params()\n",
    "\n",
    "# Create a new MLflow Experiment\n",
    "mlflow.set_experiment(f\"{ml_config.mlflow_experiment_name}_ModelName_{ml_config.model_name}\",\n",
    "#                      description=ml_config.description\n",
    "                     )\n",
    "\n",
    "#set tags\n",
    "tags = {\n",
    "        'model_name': ml_config.model_name,\n",
    "        'dataset_filename':ml_config.file,\n",
    "        \"no_enc_ppid_experiment\" : ml_config.no_enc_ppid_experiment,\n",
    "    \"low_feature_count_less_than_5_bmi_exclude\" : ml_config.low_feature_count_less_than_5_bmi_exclude,\n",
    "    'no_enc_after_2_plus_vi_above_50_exlcude':no_enc_after_2_plus_vi_above_50_exlcude,\n",
    "    \n",
    "    \"pat_before_index_visit_dia_meds_remove\" : ml_config.pat_before_index_visit_dia_meds_remove,\n",
    "    \"pat_outcome0_with_dia_meds_remove\" : ml_config.pat_outcome0_with_dia_meds_remove,\n",
    "    \n",
    "    \"sdoh_screening_experiment\" : ml_config.sdoh_screening_experiment\n",
    "    }\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run(run_name = ml_config.run_name,\n",
    "                     tags=tags):\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Log train and test PIDs\n",
    "    with io.StringIO() as f:\n",
    "        json.dump(train_pids, f)\n",
    "        f.seek(0)\n",
    "        mlflow.log_text(f.read(), \"train_pids.json\")\n",
    "\n",
    "    with io.StringIO() as f:\n",
    "        json.dump(test_pids, f)\n",
    "        f.seek(0)\n",
    "        mlflow.log_text(f.read(), \"test_pids.json\")\n",
    "    \n",
    "    # Log the hyperparameters   \n",
    "    mlflow.log_params(params)\n",
    "    \n",
    "\n",
    "    # Log the loss metric\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"roc_auc\", roc_auc)\n",
    "    mlflow.log_metric(\"sensitivity\", sensitivity)\n",
    "    mlflow.log_metric(\"specificity\", specificity)\n",
    "    mlflow.log_metric(\"auc_pr\", auc_pr)\n",
    "    mlflow.log_dict(report, \"classification_report.json\")\n",
    "    \n",
    "    \n",
    "    log_target_distribution(y_train, 'train')\n",
    "    log_target_distribution(y_test, 'test')\n",
    "    \n",
    "    mlflow.log_figure(fig, \"confusion_matrix.png\")\n",
    "    plt.close(fig)\n",
    "    \n",
    "    ### Fairnesss metrics\n",
    "    mlflow.log_metric(\"demographic parity difference for RACE\", dp_diff)\n",
    "    mlflow.log_metric(\"Equalized odds difference for RACE\", eo_diff)\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a602549e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mlflow ui"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e065faf",
   "metadata": {},
   "source": [
    "# Model performance who didn't develop in 5 year but later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46575952",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_after_5 = data_test.with_columns(pl.Series('model_prediction',model.predict(X_test)))\n",
    "data_test_after_5 = data_test_after_5.with_columns(pl.Series('model_prediction_prob',model.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e5cb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_after_5 = data_test.filter((pl.col('Outcome') == 0) &  \n",
    "                 (~pl.col('FirstOutcomeDate').is_null())\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785970e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(accuracy_score(data_test_after_5['Outcome'].to_list(), data_test_after_5['model_prediction'].to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d6450b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c1fdec3",
   "metadata": {},
   "source": [
    "# Model Performance over years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe01cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = data_test.with_columns(pl.Series('model_prediction',model.predict(X_test)))\n",
    "data_test = data_test.with_columns(pl.Series('model_prediction_prob',model.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2825428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def adjust_prediction(df: pl.DataFrame) -> pl.DataFrame:\n",
    "#     return df.with_columns(\n",
    "#         pl.when(pl.col(\"Outcome\") == 0)\n",
    "#         .then(1 - pl.col(\"model_prediction_prob\"))\n",
    "#         .otherwise(pl.col(\"model_prediction_prob\"))\n",
    "#         .alias(\"model_prediction_prob\")\n",
    "#     )\n",
    "\n",
    "# data_test = adjust_prediction(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0365e414",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.filter((pl.col('Outcome') == 0) &  (pl.col('model_prediction_prob') >0.95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102f8274",
   "metadata": {},
   "outputs": [],
   "source": [
    "req_test_df = data_test.select(['PATIENT_NUM','FirstOutcomeDate',\n",
    "                                'Outcome','model_prediction','model_prediction_prob']).filter(pl.col('Outcome') == 1)\n",
    "len(req_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c15720",
   "metadata": {},
   "outputs": [],
   "source": [
    "req_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7671ae0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(data_test['Outcome'], return_counts =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b1e305",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(data_test.filter(pl.col('model_prediction_prob')>0.9)['Outcome'].to_list(), return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b56f6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "before_04_2020_prob_09 = data_test.filter(pl.col('model_prediction_prob')>0.97)\n",
    "\n",
    "\n",
    "print(classification_report(before_04_2020_prob_09['Outcome'].to_list(), before_04_2020_prob_09['model_prediction'].to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8077f65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65fee6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "req_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa73a441",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define the date ranges\n",
    "import datetime\n",
    "\n",
    "date_ranges = [\n",
    "    (datetime.date(2016, 4, 1), datetime.date(2018, 4, 1)),\n",
    "    (datetime.date(2018, 4, 1), datetime.date(2019, 4, 1)),\n",
    "    (datetime.date(2019, 4, 1), datetime.date(2020, 4, 1)),\n",
    "    (datetime.date(2020, 4, 1), datetime.date(2021, 4, 1)),\n",
    "    (datetime.date(2021, 4, 1), datetime.date(2022, 4, 1)),\n",
    "]\n",
    "\n",
    "sum_pat = 0\n",
    "for start, end in date_ranges:\n",
    "    print(start)\n",
    "    df = req_test_df.filter(\n",
    "        (pl.col(\"FirstOutcomeDate\") >= pl.lit(start)) &\n",
    "        (pl.col(\"FirstOutcomeDate\") < pl.lit(end))\n",
    "    )\n",
    "    \n",
    "    accuracy = (\n",
    "    df.select(\n",
    "        (pl.col(\"Outcome\") == pl.col(\"model_prediction\")).alias(\"correct\")\n",
    "    )\n",
    "    .select(pl.col(\"correct\").mean())\n",
    "    .to_numpy()[0]\n",
    "    )\n",
    "    plt.hist(df.filter(pl.col('Outcome')==1)['model_prediction_prob'].to_list(), bins=20)  # 'bins' can be changed as needed\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Probability distribution of class 1')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Number of data points: \", len(df))\n",
    "    \n",
    "    sum_pat+= len(df)\n",
    "    print(\"Accuracy: \",accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6d25dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_pat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62721b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "req_test_df_2 = data_test.select(['PATIENT_NUM','FirstOutcomeDate',\n",
    "                                'Outcome','model_prediction']).filter((pl.col('Outcome') == 0) & \n",
    "                                                                      (~pl.col('FirstOutcomeDate').is_null()))\n",
    "req_test_df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317abbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(req_test_df_2.filter(pl.col('Outcome')==pl.col('model_prediction')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb345790",
   "metadata": {},
   "source": [
    "# Pre-diabetes and general population performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9ed838",
   "metadata": {},
   "outputs": [],
   "source": [
    "pids_dia_loincs_values  = pl.read_parquet('Determine_Outcome1_dia_labloinc_values.parquet')\n",
    "print(\"Number of data points with data available for determining pre-diabetes or not: \",len(pids_dia_loincs_values))\n",
    "pids_dia_loincs_values.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcf50d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.filter(pl.col('PATIENT_NUM').is_in(pids_dia_loincs_values['PATIENT_NUM'].to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d69d61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pids_dia_loincs_values.filter(\n",
    "    pl.any_horizontal(\n",
    "        pl.col(col) > 10 for col in pids_dia_loincs_values.columns[1:]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97a65b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_check = pids_dia_loincs_values.columns[1:]  # Dynamically get all except the first\n",
    "\n",
    "pids_pre_diabetic = pids_dia_loincs_values.filter(\n",
    "    pl.any_horizontal(\n",
    "        ((pl.col(col) >= 5.7) & (pl.col(col) <= 6.4)) for col in pids_dia_loincs_values.columns[1:]\n",
    "    ))['PATIENT_NUM'].to_list()\n",
    "\n",
    "pids_normal_value = pids_dia_loincs_values.filter(\n",
    "    pl.any_horizontal(\n",
    "        pl.col(col) < 5.7 for col in pids_dia_loincs_values.columns[1:]\n",
    "    ))['PATIENT_NUM'].to_list()\n",
    "\n",
    "pids_diab_value = pids_dia_loincs_values.filter(\n",
    "    pl.any_horizontal(\n",
    "        pl.col(col) > 6.5 for col in pids_dia_loincs_values.columns[1:]\n",
    "    ))['PATIENT_NUM'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f15d76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pids_diab_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a8464f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pids_normal_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7f5381",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_pre_dai = data_test.filter(pl.col('PATIENT_NUM').is_in(pids_pre_diabetic))\n",
    "dt_normal = data_test.filter(pl.col('PATIENT_NUM').is_in(pids_normal_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02d1f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dt_pre_dai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7040da92",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(data_test_pids_fp) & set(dt_pre_dai['PATIENT_NUM'].to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14148d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(dt_pre_dai['Outcome'].to_list(), return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61de54ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FOR pre-iabetic patient\n",
    "report = classification_report(dt_pre_dai['Outcome'].to_list(), dt_pre_dai['model_prediction'].to_list())\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5722fb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FOR normal range patients\n",
    "report = classification_report(dt_normal['Outcome'].to_list(), dt_normal['model_prediction'].to_list())\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6eac5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(dt_pre_dai['Outcome'].to_list(), dt_pre_dai['model_prediction'].to_list())\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d329a265",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pre_dia_df = data_test.filter(pl.col('PATIENT_NUM').is_in(pids_pre_diabetic))\n",
    "print('Number of patients who are pre-diabetes and diagnosed with diabetes: ',len(test_pre_dia_df))\n",
    "test_normal_df = data_test.filter(pl.col('PATIENT_NUM').is_in(pids_normal_value))\n",
    "print('Number of patients who are normal and got diagnosed with diabetes: ',len(test_normal_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9721b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pre_dia_df.filter((pl.col(\"Outcome\") == pl.col(\"model_prediction\")) & (pl.col(\"Outcome\") == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dae122b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "167ad617",
   "metadata": {},
   "source": [
    "# SHaP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8fe3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e67ab56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SHAP explainer  \n",
    "explainer = shap.TreeExplainer(model, X_train)\n",
    "\n",
    "# Calculate SHAP values\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# Plot the summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8908e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_test, max_display=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a1edff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aed4923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the summary\n",
    "shap.summary_plot(shap_values, X_test, max_display= 10, show=False)\n",
    "\n",
    "# Now get current axes and set y-tick labels manually\n",
    "ax = plt.gca()\n",
    "ax.set_yticklabels(['Age group', 'BMI', 'Gender: women', 'Percent of adults age >25 who graduated from college', \n",
    "                    'Gender: man','Hispanic: yes', 'Glucose [Mass/volume] in Serum or Plasma', 'Race: white', 'Diastolic blood pressure',\n",
    "                   'Median household income'][::-1])  # Put your custom labels here\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ee7e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'shap_values_{ml_config.model_name}_{ml_config.file.split(\".\")[0]}.pkl', 'wb') as f:\n",
    "    pickle.dump(shap_values, f)\n",
    "# \n",
    "# Loading the explainer later\n",
    "with open(f'shap_values_{ml_config.model_name}_{ml_config.file.split(\".\")[0]}.pkl', 'rb') as f:\n",
    "    shap_values = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e78e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_config.file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc86294c",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(model, X_test[:75000])\n",
    "\n",
    "# Calculate SHAP values\n",
    "shap_values = explainer(X_test[:75000])\n",
    "\n",
    "# Plot the summary\n",
    "shap.summary_plot(shap_values, X_test[:75000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a3aa97",
   "metadata": {},
   "source": [
    "## Individual experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff70ed7a",
   "metadata": {},
   "source": [
    "## Bias From Gender_CD column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a96b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fairlearn -q\n",
    "\n",
    "from fairlearn.metrics import MetricFrame, selection_rate, demographic_parity_difference, equalized_odds_difference\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10fb0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier(#**best_params, \n",
    "                          scale_pos_weight = (len(y_train) - sum(y_train)) / sum(y_train),  \n",
    "#                           use_label_encoder=False,\n",
    "                          device = 'cuda', \n",
    "                          )\n",
    "\n",
    "model.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a48e774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_collapse(row):\n",
    "    if row.sum() == 0:\n",
    "        return -1\n",
    "    return np.argmax(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cbc743",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitive_feature_df = X_test[[col for col in X_train.columns if 'Gender' in col]]\n",
    "\n",
    "\n",
    "\n",
    "sensitive_feature = sensitive_feature_df.apply(row_collapse, axis =1)\n",
    "print('For gender')\n",
    "metricframe = MetricFrame(\n",
    "    metrics={\n",
    "        'accuracy': accuracy_score,\n",
    "        'selection_rate': selection_rate\n",
    "    },\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred,\n",
    "    sensitive_features=sensitive_feature\n",
    ")\n",
    "\n",
    "print(metricframe.by_group)\n",
    "\n",
    "# Demographic parity difference (difference in positive rate between groups)\n",
    "dp_diff = demographic_parity_difference(y_test, y_pred, sensitive_features=sensitive_feature)\n",
    "print(\"Demographic Parity Difference:\", dp_diff)\n",
    "\n",
    "# Equalized odds difference (difference in TPR/FPR between groups)\n",
    "eo_diff = equalized_odds_difference(y_test, y_pred, sensitive_features=sensitive_feature)\n",
    "print(\"Equalized Odds Difference:\", eo_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35d774f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier(#**best_params, \n",
    "                          scale_pos_weight = (len(y_train) - sum(y_train)) / sum(y_train),  \n",
    "#                           use_label_encoder=False,\n",
    "                          device = 'cuda', \n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd794924",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rm_gen = X_train.drop(['Gender_CD_M', 'Gender_CD_W'], axis =1)\n",
    "X_test_rm_gen = X_test.drop(['Gender_CD_M', 'Gender_CD_W'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65349f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_rm_gen, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad595424",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_rm_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc857933",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973565f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "metricframe = MetricFrame(\n",
    "    metrics={\n",
    "        'accuracy': accuracy_score,\n",
    "        'selection_rate': selection_rate\n",
    "    },\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred,\n",
    "    sensitive_features=sensitive_feature\n",
    ")\n",
    "\n",
    "print(metricframe.by_group)\n",
    "\n",
    "# Demographic parity difference (difference in positive rate between groups)\n",
    "dp_diff = demographic_parity_difference(y_test, y_pred, sensitive_features=sensitive_feature)\n",
    "print(\"Demographic Parity Difference:\", dp_diff)\n",
    "\n",
    "# Equalized odds difference (difference in TPR/FPR between groups)\n",
    "eo_diff = equalized_odds_difference(y_test, y_pred, sensitive_features=sensitive_feature)\n",
    "print(\"Equalized Odds Difference:\", eo_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11da237c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SHAP explainer  \n",
    "explainer = shap.TreeExplainer(model, X_train_rm_gen)\n",
    "\n",
    "# Calculate SHAP values\n",
    "shap_values = explainer(X_test_rm_gen)\n",
    "\n",
    "# Plot the summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a537778",
   "metadata": {},
   "source": [
    "### Performance on before april 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792ad27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d03a2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_before_apr2020 = data_test.filter((pl.col('FirstOutcomeDate').is_null()) |\n",
    "                (pl.col('FirstOutcomeDate') < pl.lit('2019-04-01').str.strptime(pl.Datetime, \"%Y-%m-%d\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca0b3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_before_apr2020.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4b669f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(test_before_apr2020['Outcome'].to_list(), return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322dc89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_before_2020_set = test_before_apr2020.drop(['PATIENT_NUM','FirstOutcomeDate','Outcome', 'mode_height']).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0973fde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics = get_metrics(model,test_df_before_2020_set, test_before_apr2020['Outcome'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e4c76c",
   "metadata": {},
   "source": [
    "### Experiment: Understanding how low feature count features are classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b770a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Experiment related files/pids_1_feat_bmi_missing.pkl','rb') as f:\n",
    "    pids_1_feat_bmi_missing = pickle.load(f).to_list()\n",
    "print(\"Number of patient IDs:\", len(pids_1_feat_bmi_missing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149da5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pids_1_feat_bmi_missing[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdd3065",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_with_index = data_test.with_row_index(\"row_nr\", offset=1) \n",
    "data_test_with_index.filter(pl.col('PATIENT_NUM').is_in(pids_1_feat_bmi_missing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ffefbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.filter(pl.col('PATIENT_NUM') == 2386950).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515a1484",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shap.plots.waterfall(shap_values[9], max_display=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99185a4",
   "metadata": {},
   "source": [
    "#### Hypothesis: removing data points which have feature count 1, bmi and bp missing, will improve perofrmance of test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78229bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.filter(pl.col('PATIENT_NUM').is_in(pids_1_feat_bmi_missing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11124798",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data train before removing pids:\",len(data_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bfff4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_train.filter(~pl.col('PATIENT_NUM').is_in(pids_1_feat_bmi_missing))\n",
    "# data_test = data_test.filter(~pl.col('PATIENT_NUM').is_in(pids_1_feat_bmi_missing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35197de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data train after removing pids:\",len(data_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc3b696",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,y_train = data_train.drop(['PATIENT_NUM','FirstOutcomeDate','Outcome']).to_pandas(), data_train['Outcome'].to_pandas()\n",
    "X_test,y_test = data_test.drop(['PATIENT_NUM','FirstOutcomeDate','Outcome']).to_pandas(), data_test['Outcome'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ee47a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostClassifier( auto_class_weights='Balanced',allow_writing_files=False,\n",
    "                                   task_type ='GPU',\n",
    "                                   silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff43a7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76071b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Metrics\n",
    "conf_matrix, accuracy, roc_auc, sensitivity, specificity, auc_pr, cm = get_metrics(model,X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e13b929",
   "metadata": {},
   "source": [
    "## TP, TN, FN and FP indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703ce839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model  = CatBoostClassifier().load_model('../Determine_trained_models/Catboost'+ ml_config.file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60117d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'model' is your trained model and it has a predict method\n",
    "# Predict on the train data\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Boolean array where True indicates false positives\n",
    "false_positives_mask = (predictions == 1) & (y_test == 0)\n",
    "false_negatives_mask = (predictions == 0) & (y_test == 1)\n",
    "true_positives_mask = (predictions == 1) & (y_test == 1)\n",
    "true_negatives_mask = (predictions == 0) & (y_test == 0)\n",
    "# Get indices of false positives\n",
    "false_positive_indices = np.where(false_positives_mask)[0]\n",
    "false_negatives_indices = np.where(false_negatives_mask)[0]\n",
    "true_positives_indices = np.where(true_positives_mask)[0]\n",
    "true_negatives_indices = np.where(true_negatives_mask)[0]\n",
    "\n",
    "print(\"Indices of False Positives:\", false_positive_indices)\n",
    "print(\"Indices of False Negatives:\", false_negatives_indices)\n",
    "print(\"Indices of True Positives:\", true_positives_indices)\n",
    "print(\"Indices of True Negatives:\", true_negatives_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82af6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(false_positive_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8773a57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(false_negatives_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2745b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_pids_fp = data_test[false_positive_indices]['PATIENT_NUM'].to_list()\n",
    "data_test_pids_fn = data_test[false_negatives_indices]['PATIENT_NUM'].to_list()\n",
    "data_test_pids_tp = data_test[true_positives_indices]['PATIENT_NUM'].to_list()\n",
    "data_test_pids_tn = data_test[true_negatives_indices]['PATIENT_NUM'].to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090f6e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = f\"./FP_FN_{ml_config.file.split('.')[0]}\"\n",
    "\n",
    "# Create the directory\n",
    "os.makedirs(directory_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4714f56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "labels = {\n",
    "    'false_positives': data_test_pids_fp,\n",
    "    'false_negatives': data_test_pids_fn,\n",
    "    'true_positives': data_test_pids_tp,\n",
    "    'true_negatives': data_test_pids_tn\n",
    "}\n",
    "\n",
    "# Save all\n",
    "for label, data in labels.items():\n",
    "    with open(f'./{directory_path}/{ml_config.model_name}_{label}_pids_test.pkl', 'wb') as file:\n",
    "        pickle.dump(data, file)\n",
    "\n",
    "# Load all\n",
    "loaded = {}\n",
    "for label in labels:\n",
    "    with open(f'./{directory_path}/{ml_config.model_name}_{label}_pids_test.pkl', 'rb') as file:\n",
    "        loaded[label] = pickle.load(file)\n",
    "\n",
    "# Unpack if needed\n",
    "data_test_pids_fp = loaded['false_positives']\n",
    "data_test_pids_fn = loaded['false_negatives']\n",
    "data_test_pids_tp = loaded['true_positives']\n",
    "data_test_pids_tn = loaded['true_negatives']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586ea2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4562241",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.filter(pl.col('PATIENT_NUM').is_in(data_test_pids_fn)).sort('FirstOutcomeDate').filter(pl.col(\"FirstOutcomeDate\") < datetime(2019, 1, 2))\n",
    "          #,return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c623075",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.filter(pl.col('PATIENT_NUM').is_in(data_test_pids_fp)).filter(pl.col(\"Outcome\")== 1)\n",
    "          #,return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18790465",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[[36, 54]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f04119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "421cb595",
   "metadata": {},
   "source": [
    "# Probability Distribution across FP, FN, TN and TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2edd961",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Boolean arrays\n",
    "false_positives_mask = (predictions == 1) & (y_test == 0)\n",
    "false_negatives_mask = (predictions == 0) & (y_test == 1)\n",
    "true_positives_mask = (predictions == 1) & (y_test == 1)\n",
    "true_negatives_mask = (predictions == 0) & (y_test == 0)\n",
    "\n",
    "# Indices for each group\n",
    "false_positive_indices = np.where(false_positives_mask)[0]\n",
    "false_negatives_indices = np.where(false_negatives_mask)[0]\n",
    "true_positives_indices = np.where(true_positives_mask)[0]\n",
    "true_negatives_indices = np.where(true_negatives_mask)[0]\n",
    "\n",
    "print(\"Indices of False Positives:\", false_positive_indices)\n",
    "print(\"Indices of False Negatives:\", false_negatives_indices)\n",
    "print(\"Indices of True Positives:\", true_positives_indices)\n",
    "print(\"Indices of True Negatives:\", true_negatives_indices)\n",
    "\n",
    "# Probability predictions\n",
    "x_test_prob = model.predict_proba(X_test)\n",
    "\n",
    "# Probability distributions for each group\n",
    "fp_probs = x_test_prob[false_positive_indices]  # False positives\n",
    "fn_probs = x_test_prob[false_negatives_indices] # False negatives\n",
    "tp_probs = x_test_prob[true_positives_indices]  # True positives\n",
    "tn_probs = x_test_prob[true_negatives_indices]  # True negatives\n",
    "\n",
    "print(\"Probabilities for False Positives:\", fp_probs)\n",
    "print(\"Probabilities for False Negatives:\", fn_probs)\n",
    "print(\"Probabilities for True Positives:\", tp_probs)\n",
    "print(\"Probabilities for True Negatives:\", tn_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427cd7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract probability assigned to positive class (class 1)\n",
    "fp_probs_pos = fp_probs[:, 1]\n",
    "fn_probs_pos = fn_probs[:, 0]\n",
    "tp_probs_pos = tp_probs[:, 1]\n",
    "tn_probs_pos = tn_probs[:, 0]\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(fp_probs_pos, bins=20, alpha=0.6, label='False Positives', color='red', density=True)\n",
    "plt.hist(fn_probs_pos, bins=20, alpha=0.6, label='False Negatives', color='blue', density=True)\n",
    "plt.hist(tp_probs_pos, bins=20, alpha=0.6, label='True Positives', color='green', density=True)\n",
    "plt.hist(tn_probs_pos, bins=20, alpha=0.6, label='True Negatives', color='gray', density=True)\n",
    "plt.xlabel('Predicted Probability of Positive Class')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Histogram of Predicted Probabilities by Prediction Type')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812fa394",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "plt.suptitle('Probability distribution across FP, FN, TN and TP')\n",
    "# False Positives\n",
    "axes[0, 0].hist(fp_probs_pos, bins=20, color='red')\n",
    "axes[0, 0].set_title('False Positives')\n",
    "axes[0, 0].set_xlabel('Predicted Probability (Positive Class)')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# False Negatives\n",
    "axes[0, 1].hist(fn_probs_pos, bins=20, color='blue')\n",
    "axes[0, 1].set_title('False Negatives')\n",
    "axes[0, 1].set_xlabel('Predicted Probability (Positive Class)')\n",
    "axes[0, 1].set_ylabel('Count')\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# True Positives\n",
    "axes[1, 0].hist(tp_probs_pos, bins=20, color='green')\n",
    "axes[1, 0].set_title('True Positives')\n",
    "axes[1, 0].set_xlabel('Predicted Probability (Positive Class)')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# True Negatives\n",
    "axes[1, 1].hist(tn_probs_pos, bins=20, color='gray')\n",
    "axes[1, 1].set_title('True Negatives')\n",
    "axes[1, 1].set_xlabel('Predicted Probability (Positive Class)')\n",
    "axes[1, 1].set_ylabel('Count')\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144452c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get predicted probabilities for positive class\n",
    "y_prob = model.predict_proba(X_test)[:, 1]\n",
    "thresholds = np.linspace(0, 1, 101)\n",
    "\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1s = []\n",
    "\n",
    "for thresh in thresholds:\n",
    "    y_pred_thresh = (y_prob >= thresh).astype(int)\n",
    "    accuracies.append(accuracy_score(y_test, y_pred_thresh))\n",
    "    precisions.append(precision_score(y_test, y_pred_thresh, zero_division=0))\n",
    "    recalls.append(recall_score(y_test, y_pred_thresh))\n",
    "    f1s.append(f1_score(y_test, y_pred_thresh))\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(thresholds, accuracies, label='Accuracy')\n",
    "plt.plot(thresholds, precisions, label='Precision')\n",
    "plt.plot(thresholds, recalls, label='Recall')\n",
    "plt.plot(thresholds, f1s, label='F1 Score')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Model Performance at Different Thresholds')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80941dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict class and probabilities\n",
    "y_pred = model.predict(X_test)\n",
    "y_prob = model.predict_proba(X_test)[:, 0]\n",
    "\n",
    "\n",
    "false_negatives_mask = (y_pred == 0) & (y_test == 1)\n",
    "\n",
    "\n",
    "high_prob_mask = y_prob > 0.90\n",
    "\n",
    "\n",
    "fn_high_prob_mask = false_negatives_mask & high_prob_mask\n",
    "\n",
    "fn_high_prob_indices = np.where(fn_high_prob_mask)[0]\n",
    "\n",
    "# Get data points and optionally their probabilities\n",
    "X_fn_high_prob = X_test.iloc[fn_high_prob_indices]\n",
    "probs_fn_high = y_prob[fn_high_prob_indices]\n",
    "\n",
    "print(\"Indices of FNs with prob > 0.90:\", fn_high_prob_indices)\n",
    "print(\"Probs of those:\", probs_fn_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c765279",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[fn_high_prob_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ef32d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict class and probabilities\n",
    "y_pred = model.predict(X_test)\n",
    "y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "\n",
    "false_positives_mask = (y_pred == 1) & (y_test == 0)\n",
    "\n",
    "\n",
    "high_prob_mask = y_prob > 0.90\n",
    "\n",
    "\n",
    "fp_high_prob_mask = false_positives_mask & high_prob_mask\n",
    "\n",
    "fp_high_prob_indices = np.where(fp_high_prob_mask)[0]\n",
    "\n",
    "# Get data points and optionally their probabilities\n",
    "X_fp_high_prob = X_test.iloc[fp_high_prob_indices]\n",
    "probs_fp_high = y_prob[fp_high_prob_indices]\n",
    "\n",
    "print(\"Indices of FNs with prob > 0.90:\", fp_high_prob_indices)\n",
    "print(\"Probs of those:\", probs_fp_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a284149",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[fp_high_prob_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df30c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict class and probabilities\n",
    "y_pred = model.predict(X_test)\n",
    "y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "\n",
    "true_positives_mask = (y_pred == 1) & (y_test == 1)\n",
    "\n",
    "\n",
    "high_prob_mask = y_prob > 0.90\n",
    "\n",
    "\n",
    "tp_high_prob_mask = true_positives_mask & high_prob_mask\n",
    "\n",
    "tp_high_prob_indices = np.where(tp_high_prob_mask)[0]\n",
    "\n",
    "# Get data points and optionally their probabilities\n",
    "X_tp_high_prob = X_test.iloc[tp_high_prob_indices]\n",
    "probs_tp_high = y_prob[tp_high_prob_indices]\n",
    "\n",
    "print(\"Indices of FNs with prob > 0.90:\", tp_high_prob_indices)\n",
    "print(\"Probs of those:\", probs_tp_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f1649e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[tp_high_prob_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea06cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# True Negatives mask (predicted 0, actual 0)\n",
    "true_negatives_mask = (y_pred == 0) & (y_test == 0)\n",
    "\n",
    "# High confidence negatives: probability of class 1 is LOW (< 0.10)\n",
    "low_prob_mask = y_prob < 0.10\n",
    "\n",
    "# TN with high probability (really low proba for class 1)\n",
    "tn_high_conf_mask = true_negatives_mask & low_prob_mask\n",
    "\n",
    "tn_high_conf_indices = np.where(tn_high_conf_mask)[0]\n",
    "\n",
    "# Get data points and their probabilities\n",
    "X_tn_high_conf = X_test.iloc[tn_high_conf_indices]\n",
    "probs_tn_high = y_prob[tn_high_conf_indices]\n",
    "\n",
    "print(\"Indices of TNs with prob < 0.10:\", tn_high_conf_indices)\n",
    "print(\"Probs of those:\", probs_tn_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48ddaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[tn_high_conf_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc380d73",
   "metadata": {},
   "source": [
    "### Loooking to waterfall plots for >90 prob tn, tp, fn and fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2d8ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_missing_df = pl.read_parquet('../Determine_missing_features_OCHIN.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b4c574",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_missing_df.filter(pl.col('PATIENT_NUM').is_in(data_test[fn_high_prob_indices]['PATIENT_NUM'].to_list())).filter(pl.col('Total_Feature_Count')!=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d017509",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_missing_df.filter(pl.col('PATIENT_NUM').is_in(data_test[fn_high_prob_indices]['PATIENT_NUM'].to_list())).filter(pl.col('Total_Feature_Count')<40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e2f6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the index of the row with patient_num 4321905\n",
    "data_test.with_row_index().filter(pl.col('PATIENT_NUM')==4314903)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b315751",
   "metadata": {},
   "source": [
    "# Waterfall plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03ff870",
   "metadata": {},
   "source": [
    "## FN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c754d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "patient_nums_of_interest = (\n",
    "    feature_missing_df\n",
    "    .filter(\n",
    "        pl.col('PATIENT_NUM').is_in(\n",
    "            data_test[fn_high_prob_indices]['PATIENT_NUM'].to_list()\n",
    "#         ) & (pl.col('Total_Feature_Count') < 10)\n",
    "    ))\n",
    "    .select('PATIENT_NUM')\n",
    "    .to_series()\n",
    "    .to_list()\n",
    ")\n",
    "\n",
    "# Add row index to data_test for easy lookup\n",
    "test_with_index = data_test.with_row_index(name=\"ROW_INDEX\")\n",
    "\n",
    "# Map each PATIENT_NUM to its row index\n",
    "patnum_to_index = dict(\n",
    "    test_with_index\n",
    "    .filter(pl.col('PATIENT_NUM').is_in(patient_nums_of_interest))\n",
    "    .select(['PATIENT_NUM', 'ROW_INDEX'])\n",
    "    .rows()\n",
    ")\n",
    "\n",
    "# Plot SHAP waterfall plot for each index\n",
    "for pat_num, idx in patnum_to_index.items():\n",
    "    print(f\"Waterfall plot for PATIENT_NUM {pat_num} (row index {idx}):\")\n",
    "    print(\"Total feature count(meds, labs and dxs): \",feature_missing_df.filter(pl.col('PATIENT_NUM')==pat_num)['Total_Feature_Count'][0])\n",
    "    shap.plots.waterfall(shap_values[idx])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86267e2b",
   "metadata": {},
   "source": [
    "## FP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0862d396",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "patient_nums_of_interest = (\n",
    "    feature_missing_df\n",
    "    .filter(\n",
    "        pl.col('PATIENT_NUM').is_in(\n",
    "            data_test[fp_high_prob_indices]['PATIENT_NUM'].to_list()\n",
    "        ) \n",
    "        \n",
    "#         & (pl.col('Total_Feature_Count') != 1)\n",
    "    )\n",
    "    .select('PATIENT_NUM')\n",
    "    .to_series()\n",
    "    .to_list()\n",
    ")\n",
    "\n",
    "# Add row index to data_test for easy lookup\n",
    "test_with_index = data_test.with_row_index(name=\"ROW_INDEX\")\n",
    "\n",
    "# Map each PATIENT_NUM to its row index\n",
    "patnum_to_index = dict(\n",
    "    test_with_index\n",
    "    .filter(pl.col('PATIENT_NUM').is_in(patient_nums_of_interest))\n",
    "    .select(['PATIENT_NUM', 'ROW_INDEX'])\n",
    "    .rows()\n",
    ")\n",
    "\n",
    "# Plot SHAP waterfall plot for each index\n",
    "for pat_num, idx in patnum_to_index.items():\n",
    "    print(f\"Waterfall plot for PATIENT_NUM {pat_num} (row index {idx}):\")\n",
    "    print(\"Total feature count(meds, labs and dxs): \",feature_missing_df.filter(pl.col('PATIENT_NUM')==pat_num)['Total_Feature_Count'][0])\n",
    "    shap.plots.waterfall(shap_values[idx])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd84cf8",
   "metadata": {},
   "source": [
    "# TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8df9b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "patient_nums_of_interest = (\n",
    "    feature_missing_df\n",
    "    .filter(\n",
    "        pl.col('PATIENT_NUM').is_in(\n",
    "            data_test[tp_high_prob_indices]['PATIENT_NUM'].to_list()[:200]\n",
    "        ) & (pl.col('Total_Feature_Count') >20)\n",
    "    )\n",
    "    .select('PATIENT_NUM')\n",
    "    .to_series()\n",
    "    .to_list()\n",
    ")\n",
    "\n",
    "# Add row index to data_test for easy lookup\n",
    "test_with_index = data_test.with_row_index(name=\"ROW_INDEX\")\n",
    "\n",
    "# Map each PATIENT_NUM to its row index\n",
    "patnum_to_index = dict(\n",
    "    test_with_index\n",
    "    .filter(pl.col('PATIENT_NUM').is_in(patient_nums_of_interest))\n",
    "    .select(['PATIENT_NUM', 'ROW_INDEX'])\n",
    "    .rows()\n",
    ")\n",
    "\n",
    "# Plot SHAP waterfall plot for each index\n",
    "for pat_num, idx in patnum_to_index.items():\n",
    "    print(f\"Waterfall plot for PATIENT_NUM {pat_num} (row index {idx}):\")\n",
    "    print(\"Total feature count(meds, labs and dxs): \",feature_missing_df.filter(pl.col('PATIENT_NUM')==pat_num)['Total_Feature_Count'][0])\n",
    "    shap.plots.waterfall(shap_values[idx])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f92192e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_train.with_columns(pl.Series('Outcome',y_train.values))\n",
    "\n",
    "\n",
    "# Count occurrences of each combination\n",
    "counts = data_train.group_by(['Gender_CD_W', 'Gender_CD_M', 'Outcome']).count().sort('Outcome')\n",
    "\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc9f7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = data_test.with_columns(pl.Series('Outcome',y_test.values))\n",
    "\n",
    "\n",
    "# Count occurrences of each combination\n",
    "counts_test = data_test.group_by(['Gender_CD_W', 'Gender_CD_M', 'Outcome']).count().sort('Outcome')\n",
    "\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991561d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train.drop(['Gender_CD_W', 'Gender_CD_M'], axis = 1),y_train)\n",
    "\n",
    "conf_matrix, accuracy, roc_auc, sensitivity, specificity, auc_pr, cr = get_metrics(model,X_test.drop(['Gender_CD_W', 'Gender_CD_M'], axis =1), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb135e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer1 = shap.TreeExplainer(model, X_train)\n",
    "\n",
    "# Calculate SHAP values\n",
    "shap_values1 = explainer1(X_test)\n",
    "\n",
    "# Plot the summary\n",
    "shap.summary_plot(shap_values1, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e85b92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d98cd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.waterfall(shap_values[71179])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182de181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data point with 42 features\n",
    "shap.plots.waterfall(shap_values[126669])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d342b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "## with 33 features\n",
    "shap.plots.waterfall(shap_values[69511])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ca3688",
   "metadata": {},
   "source": [
    "# Fairness metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fafef4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2d622af",
   "metadata": {},
   "source": [
    "# Sub-group performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de9f06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## model to use\n",
    "model  = CatBoostClassifier().load_model('../Determine_trained_models/catboost_whole_dataset_phecodes_bmi_bp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abd40c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.columns[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13d25c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c19a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_data(sub_x_test,sub_y_test, model):\n",
    "    y_pred = model.predict(sub_x_test)\n",
    "    y_pred_probs = model.predict_proba(sub_x_test)[:,1]\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(sub_y_test, y_pred).ravel()\n",
    "\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "\n",
    "    print(\"Sensitivity: \", sensitivity)\n",
    "    print(\"Specificity: \", specificity)\n",
    "    roc_auc = roc_auc_score(sub_y_test, y_pred_probs)\n",
    "    print(\"ROC curve: \", roc_auc)\n",
    "    \n",
    "    acc = accuracy_score(sub_y_test, y_pred)\n",
    "    print('Accuracy: ', acc)\n",
    "    return sensitivity, specificity, roc_auc, acc\n",
    "    \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15478843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - **'DEM|RACE:01'**: White\n",
    "# - **'DEM|RACE:02'**: Black or African American\n",
    "# - **'DEM|RACE:03'**: Asian\n",
    "# - **'DEM|RACE:04'**: Native Hawaiian or Other Pacific Islander\n",
    "# - **'DEM|RACE:05'**: American Indian or Alaska Native\n",
    "# - **'DEM|RACE:06'**: Other or Unknown\n",
    "# - **'DEM|RACE:07'**: More than one race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb57410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfa2f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "race_cols = [\n",
    "    \"Race_CD_01\",\n",
    "    \"Race_CD_02\",\n",
    "    \"Race_CD_03\",\n",
    "    \"Race_CD_04\",\n",
    "    \"Race_CD_05\",\n",
    "    \"Race_CD_06\",\n",
    "    \"Race_CD_07\",\n",
    "    \"Race_CD_UNK\"\n",
    "]\n",
    "\n",
    "races = []\n",
    "sens = []\n",
    "specs = []\n",
    "roc_aucs = []\n",
    "counts = []\n",
    "races = []\n",
    "class_1_count = []\n",
    "\n",
    "for col in race_cols:\n",
    "    if col.endswith('01'):\n",
    "        race = 'white'\n",
    "    if col.endswith('02'):\n",
    "        race = 'Black or African American'\n",
    "    if col.endswith('03'):\n",
    "        race = 'Asian'\n",
    "    if col.endswith('04'):\n",
    "        race = 'Native Hawaiian or Other Pacific Islander'\n",
    "    if col.endswith('05'):\n",
    "        race = 'American Indian or Alaska Native'\n",
    "    if col.endswith('06'):\n",
    "        race = 'Multiple Race'\n",
    "    if col.endswith('07'):\n",
    "        race = 'refuse to Answer'\n",
    "    elif col.endswith('UNK'):\n",
    "        race = 'UNK'\n",
    "        \n",
    "    print(\"Number of data points with race {} are: {}\".format( col, X_test[X_test[col]==1].shape[0] ))\n",
    "    \n",
    "    # Get indexes where the value is 1 for the current race column\n",
    "    X_test_race = X_test[X_test[col] == 1]\n",
    "    indexes = X_test_race.index\n",
    "    \n",
    "    # Retrieve the respective y_test data using these indexes\n",
    "    y_test_race = y_test.loc[indexes]\n",
    "    \n",
    "    sensitivity, specificity, roc_auc = predict_on_data(X_test_race, y_test_race, model)\n",
    "    races.append(race)\n",
    "    sens.append(sensitivity)\n",
    "    specs.append(specificity)\n",
    "    roc_aucs.append(roc_auc)\n",
    "    class_1_count.append(sum(y_test_race.values))\n",
    "    counts.append(X_test[X_test[col]==1].shape[0])\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5099b239",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'Race': races,\n",
    "    'counts':counts,\n",
    "    'class_1 count': class_1_count,\n",
    "    'Sensitivity': sens,\n",
    "    'Specificity': specs,\n",
    "    'ROC_AUC': roc_aucs\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e2d830",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_collapse = df.iloc[5:8]\n",
    "\n",
    "# Calculate the new collapsed row\n",
    "new_row = {\n",
    "    'Race': 'Collapsed Race',\n",
    "    'counts': to_collapse['counts'].sum(),\n",
    "    'class_1 count': to_collapse['class_1 count'].sum(),\n",
    "    'Sensitivity': to_collapse['Sensitivity'].mean(),\n",
    "    'Specificity': to_collapse['Specificity'].mean(),\n",
    "    'ROC_AUC': to_collapse['ROC_AUC'].mean()\n",
    "}\n",
    "\n",
    "# Drop the old rows and append the new row\n",
    "df = df.drop(index=range(5, 8)).append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acffd9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e957e4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_groups = np.unique(X_test['Age_group'].values)\n",
    "\n",
    "age_ranges = []\n",
    "sens = []\n",
    "specs = []\n",
    "roc_aucs = []\n",
    "counts =[]\n",
    "class_1_count = []\n",
    "\n",
    "races = []\n",
    "for age_enc in age_groups:\n",
    "    \n",
    "    if age_enc == 0:\n",
    "        age = '18-34'\n",
    "    elif age_enc == 1:\n",
    "        age = '35-44'\n",
    "    elif age_enc == 2:\n",
    "        age = '45-54'\n",
    "    elif age_enc == 3:\n",
    "        age = '54-65'\n",
    "    elif age_enc == 4:\n",
    "        age = '65-74'\n",
    "    else:\n",
    "        age = '75_older'\n",
    "        \n",
    "    print(\"Number of data points with age group {} are: {}\".format( age_enc, X_test[X_test['Age_group']== age_enc].shape[0] ))\n",
    "    \n",
    "    # Get indexes where the value is 1 for the current race column\n",
    "    X_test_race = X_test[X_test['Age_group']== age_enc]\n",
    "    indexes = X_test_race.index\n",
    "    \n",
    "    # Retrieve the respective y_test data using these indexes\n",
    "    y_test_race = y_test.loc[indexes]\n",
    "    \n",
    "    sensitivity, specificity, roc_auc = predict_on_data(X_test_race, y_test_race, model)\n",
    "    age_ranges.append(age)\n",
    "    sens.append(sensitivity)\n",
    "    specs.append(specificity)\n",
    "    roc_aucs.append(roc_auc)\n",
    "    class_1_count.append(sum(y_test_race.values))\n",
    "    counts.append(X_test[X_test['Age_group']== age_enc].shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98542fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'Age_Group': age_ranges,\n",
    "    'counts': counts,\n",
    "    'class_1 count': class_1_count,\n",
    "    'Sensitivity': sens,\n",
    "    'Specificity': specs,\n",
    "    'ROC_AUC': roc_aucs,\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f534422",
   "metadata": {},
   "source": [
    "## Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0641137",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_cols = [\"Gender_CD_GQ\", \"Gender_CD_M\", \"Gender_CD_TG\", \"Gender_CD_UNK\", \"Gender_CD_W\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d161e65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_list = []\n",
    "sens = []\n",
    "specs = []\n",
    "roc_aucs = []\n",
    "counts =[]\n",
    "class_1_count = []\n",
    "\n",
    "races = []\n",
    "for gen in gender_cols:\n",
    "    \n",
    "    if gen.endswith('GQ'):\n",
    "        gender = 'GenderQueer'\n",
    "    elif gen.endswith('M'):\n",
    "        gender = 'Man'\n",
    "    elif gen.endswith('W'):\n",
    "        gender = 'Woman'\n",
    "    elif gen.endswith('TG'):\n",
    "        gender = 'Transgender'\n",
    "    elif gen.endswith('UNK'):\n",
    "        gender = 'Unknown'\n",
    "\n",
    "        \n",
    "    print(\"Number of data points with gender group {} are: {}\".format( gender, X_test[X_test[gen]== 1].shape[0] ))\n",
    "    \n",
    "    # Get indexes where the value is 1 for the current race column\n",
    "    X_test_gen = X_test[X_test[gen]== 1]\n",
    "    indexes = X_test_gen.index\n",
    "    \n",
    "    # Retrieve the respective y_test data using these indexes\n",
    "    y_test_gen = y_test.loc[indexes]\n",
    "    \n",
    "    sensitivity, specificity, roc_auc,_ = predict_on_data(X_test_gen, y_test_gen, model)\n",
    "    gender_list.append(gender)\n",
    "    sens.append(sensitivity)\n",
    "    specs.append(specificity)\n",
    "    roc_aucs.append(roc_auc)\n",
    "    class_1_count.append(sum(y_test_gen.values))\n",
    "    counts.append(X_test[X_test[gen]== 1].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70f941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'Gender': gender_list,\n",
    "    'counts': counts,\n",
    "    'class_1 count': class_1_count,\n",
    "    'Sensitivity': sens,\n",
    "    'Specificity': specs,\n",
    "    'ROC_AUC': roc_aucs,\n",
    "})\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeea157",
   "metadata": {},
   "source": [
    "# 5-year performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29186aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model  = CatBoostClassifier().load_model('../Determine_trained_models/catboost_whole_dataset_phecodes_bmi_bp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a10928",
   "metadata": {},
   "outputs": [],
   "source": [
    "req_test_dates_df = test_data_firstoutcome_df.drop_nulls().to_pandas()\n",
    "req_test_dates_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ab219b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "start_date = pd.Timestamp('2017-04-01')\n",
    "\n",
    "# Calculate the difference in years between the Index_Start_date and the start_date if data is available\n",
    "req_test_dates_df['year_difference'] = (req_test_dates_df['FirstOutcomeDate'] - start_date).dt.days / 365.25\n",
    "\n",
    "# Filter DataFrames based on year difference, ignore NaNs as they indicate missing data in cohort_df\n",
    "df_less_than_1 = req_test_dates_df[req_test_dates_df['year_difference'] < 1].dropna()\n",
    "df_1_to_2 = req_test_dates_df[(req_test_dates_df['year_difference'] >= 1) & (req_test_dates_df['year_difference'] < 2)].dropna()\n",
    "df_2_to_3 = req_test_dates_df[(req_test_dates_df['year_difference'] >= 2) & (req_test_dates_df['year_difference'] < 3)].dropna()\n",
    "df_3_to_4 = req_test_dates_df[(req_test_dates_df['year_difference'] >= 3) & (req_test_dates_df['year_difference'] < 4)].dropna()\n",
    "df_4_to_5 = req_test_dates_df[(req_test_dates_df['year_difference'] >= 4) & (req_test_dates_df['year_difference'] < 5)].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1feabdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_ranges = []\n",
    "sens = []\n",
    "specs = []\n",
    "roc_aucs = []\n",
    "counts =[]\n",
    "class_1_count = []\n",
    "acc_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be89da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,buf_df in enumerate([df_less_than_1, df_1_to_2, df_2_to_3, df_3_to_4]):\n",
    "     # Use the index of buf_df\n",
    "    indices = buf_df.index\n",
    "    # Use the index to filter X_test\n",
    "    X_test_buf = X_test.loc[indices.intersection(X_test.index)]\n",
    "    y_test_buf = y_test.loc[indices.intersection(X_test.index)]\n",
    "    \n",
    "    if i == 0:\n",
    "        year ='Less than 1 year'\n",
    "    if i == 1:\n",
    "        year ='Between 1 and 2 year'\n",
    "    if i == 2:\n",
    "        year ='Between 2 and 3 year'\n",
    "    if i == 3:\n",
    "        year ='Between 3 and 4 year'\n",
    "    \n",
    "    print(year)\n",
    "    sensitivity, specificity, roc_auc, acc = predict_on_data(X_test_buf, y_test_buf, model)\n",
    "    year_ranges.append(year)\n",
    "    sens.append(sensitivity)\n",
    "    specs.append(specificity)\n",
    "    roc_aucs.append(roc_auc)\n",
    "    counts.append(len(y_test_buf))\n",
    "    acc_scores.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58eda881",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'Age_Group': year_ranges,\n",
    "    'counts (class 1)': counts,\n",
    "    'Accuracy (class 1)': acc_scores,\n",
    "    'Sensitivity': sens,\n",
    "    'Specificity': specs,\n",
    "    'ROC_AUC': roc_aucs,\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adcf845",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbbfbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. Fold Ensemble (catboost)\n",
    "\n",
    "import numpy as np\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score, confusion_matrix, classification_report\n",
    "\n",
    "# Parameters\n",
    "n_folds = 5\n",
    "\n",
    "# Stratified K-Fold object\n",
    "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "test_preds = []\n",
    "valid_scores = []\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(skf.split(X_train, y_train)):\n",
    "    print(f\"Training fold {fold + 1}/{n_folds}\")\n",
    "\n",
    "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[valid_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[valid_idx]\n",
    "    \n",
    "    model = CatBoostClassifier(\n",
    "        auto_class_weights='Balanced',allow_writing_files=False,\n",
    "        task_type ='GPU', \n",
    "        verbose=0\n",
    "    )\n",
    "    model.fit(X_tr, y_tr)\n",
    "    \n",
    "    val_pred = model.predict(X_val)\n",
    "    acc = accuracy_score(y_val, val_pred)\n",
    "    valid_scores.append(acc)\n",
    "    print(f\"Validation Accuracy for Fold {fold + 1}: {acc:.4f}\")\n",
    "\n",
    "    test_pred = model.predict_proba(X_test)[:, 1]\n",
    "    test_preds.append(test_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d423c343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average predictions across folds\n",
    "mean_test_pred = np.mean(test_preds, axis=0)\n",
    "final_test_pred = (mean_test_pred >= 0.5).astype(int)\n",
    "\n",
    "# Evaluate on test set if y_test is available\n",
    "test_acc = accuracy_score(y_test, final_test_pred)\n",
    "print(f\"Ensemble Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Sensitivity (Recall for class 1)\n",
    "sensitivity = recall_score(y_test, final_test_pred)\n",
    "print(f\"Sensitivity (Recall for positive class): {sensitivity:.4f}\")\n",
    "\n",
    "# Specificity (Recall for class 0)\n",
    "cm = confusion_matrix(y_test, final_test_pred)\n",
    "# cm format: [[TN, FP],\n",
    "#             [FN, TP]]\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "print(f\"Specificity (Recall for negative class): {specificity:.4f}\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, final_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c199c622",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ad4073",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode((np.array(test_preds)>0.5).astype(int)).mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9364f6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "# Majority voting\n",
    "\n",
    "final_test_pred = mode((np.array(test_preds)>0.5).astype(int)).mode\n",
    "\n",
    "# test_acc = accuracy_score(y_test, final_test_pred)\n",
    "# print(f\"Ensemble Test Accuracy (majority vote): {test_acc:.4f}\")\n",
    "# print(\"Validation Accuracies for all folds:\", valid_scores)\n",
    "# print(\"Mean Validation Accuracy:\", np.mean(valid_scores))\n",
    "\n",
    "\n",
    "# Sensitivity (Recall for class 1)\n",
    "sensitivity = recall_score(y_test, final_test_pred)\n",
    "print(f\"Sensitivity (Recall for positive class): {sensitivity:.4f}\")\n",
    "\n",
    "# Specificity (Recall for class 0)\n",
    "cm = confusion_matrix(y_test, final_test_pred)\n",
    "# cm format: [[TN, FP],\n",
    "#             [FN, TP]]\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "print(f\"Specificity (Recall for negative class): {specificity:.4f}\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, final_test_pred))\n",
    "\n",
    "# # (Rest of your code)\n",
    "# print(\"Validation Accuracies for all folds:\", valid_scores)\n",
    "# print(\"Mean Validation Accuracy:\", np.mean(valid_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64dabae",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2. Different Models Ensemble\n",
    "###    -> Avg. Probability\n",
    "###    -> Voting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc93036c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from xgboost import XGBClassifier, XGBRegressor, Booster\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "import os\n",
    "\n",
    "def load_model(model_path, model_name=None):\n",
    "\n",
    "    \n",
    "    \n",
    "    if \"xgboost\" in model_path:\n",
    "\n",
    "            model = XGBClassifier()\n",
    "            model.load_model(model_path)\n",
    " \n",
    "            return model\n",
    "    elif \"catboost\" in model_path:\n",
    "        # For CatBoost: use load_model\n",
    "        try:\n",
    "            model = CatBoostClassifier()\n",
    "            model.load_model(model_path)\n",
    "        except Exception:\n",
    "            model = CatBoostRegressor()\n",
    "            model.load_model(model_path)\n",
    "        return model\n",
    "    elif \"random_forest\" in model_path:\n",
    "        # For sklearn-based models\n",
    "        model = joblib.load(model_path)\n",
    "        return model\n",
    "    else:\n",
    "        # Try generic joblib loading as fallback\n",
    "        model = joblib.load(model_path)\n",
    "        return model\n",
    "\n",
    "\n",
    "catboost_path = \"../Determine_trained_models/catboost_dataset_Determine_joined_med_usage_lab_median_domain_expert_diag_phemap_without_icd10z_bmi_bp_cvs_ordinal_nominal_encoded_boruta_features.cbm\"\n",
    "xgboost_path = \"../Determine_trained_models/xgboost_dataset_Determine_joined_med_usage_lab_median_domain_expert_diag_phemap_without_icd10z_bmi_bp_cvs_ordinal_nominal_encoded_boruta_features.json\"\n",
    "randomforest_path = \"../Determine_trained_models/random_forest_dataset_Determine_joined_med_usage_lab_median_domain_expert_diag_phemap_without_icd10z_bmi_bp_cvs_ordinal_nominal_encoded_boruta_features.pkl\"\n",
    "\n",
    "\n",
    "catboost_model = load_model(catboost_path, model_name=\"catboost\")\n",
    "xgboost_model = load_model(xgboost_path, model_name=\"xgboost\")\n",
    "rf_model = load_model(randomforest_path, model_name=\"random_forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec4c026",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = []\n",
    "for model in [catboost_model, xgboost_model, rf_model]:\n",
    "\n",
    "    test_pred = model.predict_proba(X_test)[:, 1]\n",
    "    test_preds.append(test_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eabd847",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2c37fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average predictions across folds\n",
    "mean_test_pred = np.mean(test_preds, axis=0)\n",
    "final_test_pred = (mean_test_pred >= 0.5).astype(int)\n",
    "\n",
    "# Evaluate on test set if y_test is available\n",
    "test_acc = accuracy_score(y_test, final_test_pred)\n",
    "print(f\"Ensemble Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Sensitivity (Recall for class 1)\n",
    "sensitivity = recall_score(y_test, final_test_pred)\n",
    "print(f\"Sensitivity (Recall for positive class): {sensitivity:.4f}\")\n",
    "\n",
    "# Specificity (Recall for class 0)\n",
    "cm = confusion_matrix(y_test, final_test_pred)\n",
    "# cm format: [[TN, FP],\n",
    "#             [FN, TP]]\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "print(f\"Specificity (Recall for negative class): {specificity:.4f}\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, final_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f51a38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "# Majority voting\n",
    "\n",
    "final_test_pred = mode((np.array(test_preds)>0.5).astype(int)).mode\n",
    "\n",
    "# test_acc = accuracy_score(y_test, final_test_pred)\n",
    "# print(f\"Ensemble Test Accuracy (majority vote): {test_acc:.4f}\")\n",
    "# print(\"Validation Accuracies for all folds:\", valid_scores)\n",
    "# print(\"Mean Validation Accuracy:\", np.mean(valid_scores))\n",
    "\n",
    "\n",
    "# Sensitivity (Recall for class 1)\n",
    "sensitivity = recall_score(y_test, final_test_pred)\n",
    "print(f\"Sensitivity (Recall for positive class): {sensitivity:.4f}\")\n",
    "\n",
    "# Specificity (Recall for class 0)\n",
    "cm = confusion_matrix(y_test, final_test_pred)\n",
    "# cm format: [[TN, FP],\n",
    "#             [FN, TP]]\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "print(f\"Specificity (Recall for negative class): {specificity:.4f}\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, final_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858457ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
